<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hexo</title>
  
  
  <link href="http://zhaohongqiangsoliva.github.io/public/atom.xml" rel="self"/>
  
  <link href="http://zhaohongqiangsoliva.github.io/public/"/>
  <updated>2022-08-16T00:56:05.186Z</updated>
  <id>http://zhaohongqiangsoliva.github.io/public/</id>
  
  <author>
    <name>John Doe</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title></title>
    <link href="http://zhaohongqiangsoliva.github.io/public/2022/08/16/2021-10-02-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-04-%E5%85%8D%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E9%A2%84%E6%B5%8B/"/>
    <id>http://zhaohongqiangsoliva.github.io/public/2022/08/16/2021-10-02-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-04-%E5%85%8D%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E9%A2%84%E6%B5%8B/</id>
    <published>2022-08-16T09:18:55.589Z</published>
    <updated>2022-08-16T00:56:05.186Z</updated>
    
    <content type="html"><![CDATA[<h1 id="强化学习—免模型预测"><a href="#强化学习—免模型预测" class="headerlink" title="强化学习—免模型预测"></a>强化学习—免模型预测</h1><blockquote><p>作者：YJLAugus  博客： <a href="https://www.cnblogs.com/yjlaugus">https://www.cnblogs.com/yjlaugus</a> 项目地址：<a href="https://github.com/YJLAugus/Reinforcement-Learning-Notes%EF%BC%8C%E5%A6%82%E6%9E%9C%E6%84%9F%E8%A7%89%E5%AF%B9%E6%82%A8%E6%9C%89%E6%89%80%E5%B8%AE%E5%8A%A9%EF%BC%8C%E7%83%A6%E8%AF%B7%E7%82%B9%E4%B8%AA%E2%AD%90Star%E3%80%82">https://github.com/YJLAugus/Reinforcement-Learning-Notes，如果感觉对您有所帮助，烦请点个⭐Star。</a></p></blockquote><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在第二章<a href="https://www.yuque.com/yjlaugus/reinforcement-learning-notes/wfea35">强化学习-动态规划-DP</a>中，我们讨论了用动态规划来求解强化学习预测问题和控制问题的方法。但是由于动态规划法需要在每一次回溯更新某一个状态的价值时，回溯到该状态的所有可能的后续状态。导致对于复杂问题计算量很大。同时很多时候，我们连环境的<strong>动态特性</strong> $P$ 都无法知道，这时动态规划法根本没法使用。这时候我们如何求解强化学习问题呢？</p><p>从本章开始将花连续两讲的时间讨论解决一个可以被认为是MDP、但却不掌握MDP具体细节的问题，也就是讲述<strong>如何直接从Agent与环境的交互来得得到一个估计的最优价值函数和最优策略</strong>。这部分内容同样分为两部分，第一部分也就是本章的内容，聚焦于策略评估，也就是预测，直白的说就是在给定的策略同时不清楚MDP细节的情况下，估计Agent会得到怎样的最终奖励。下一讲将利用本章的主要观念来进行控制进而找出最优策略，最大化Agent的奖励。</p><p>本章内容分为三个小部分，分别是<strong>蒙特卡洛强化学习</strong>、<strong>时序差分强化学</strong>习和介于两者之间的<strong>λ时序差分强化学习</strong>。相信读者在阅读本讲内容后会对这三类学习算法有一定的理解。</p><p>其中在第三章<a href="https://www.yuque.com/yjlaugus/reinforcement-learning-notes/gw7d7v"><strong>蒙特卡洛（Markov Chain &amp; Monte Carlo, MCMC）方法</strong></a> 对蒙特卡洛方法进行了一个简单的介绍，这样对于这一章节问题的解决会有很多帮助。</p><h2 id="免模型的强化学习问题定义"><a href="#免模型的强化学习问题定义" class="headerlink" title="免模型的强化学习问题定义"></a>免模型的强化学习问题定义</h2><p>在动态规划法中，强化学习的两个问题是这样定义的：</p><ul><li><p>预测问题，即给定强化学习的6个要素：状态集 $S$, 动作集 $A$, 模型状态转化概率矩阵$P$, 即时奖励 $R$，衰减因子$\gamma$, 给定策略 $\pi$，求解该策略的状态价值函数 $v_{π}$。</p></li><li><p>控制问题，也就是求解最优的价值函数和策略。给定强化学习的5个要素：状态集 $S$, 动作集 $A$, 模型状态转化概率矩阵$P$, 即时奖励 $R$，衰减因子$\gamma$, 给定策略 $\pi$， 求解最优的状态价值函数$v_*$和最优策略$\pi_*$</p></li></ul><p>　</p><p>可见，<strong>模型状态转化概率矩阵$P$始终是已知的，即MDP已知</strong>，对于这样的强化学习问题，我们一般称为<strong>基于模型的强化学习问题</strong>。</p><p>不过有很多强化学习问题，我们没有办法事先得到模型状态转化概率矩阵$P$，这时如果仍然需要我们求解强化学习问题，那么这就是不基于模型的强化学习问题了——<strong>免模型的强化学习</strong>。它的两个问题一般的定义是：　　　　</p><ul><li><p>预测问题，即给定强化学习的5个要素：状态集$S$, 动作集$A$ 即时奖励$R$，衰减因子$\gamma$，给定策略$\pi$， 求解该策略的状态价值函数$v_\pi$</p></li><li><p>控制问题，也就是求解最优价值函数和策略。给定强化学习的5个要素：状态集$S$, 动作集$A$, 即时奖励$R$，衰减因子$\gamma$, <strong>探索率 $\epsilon$</strong> , 求解最优的动作价值函数$q_*$和最优策略$\pi_*$</p></li></ul><p>　</p><p>本章节要讨论的蒙特卡洛方法就是上述免模型的强化学习问题。</p><h2 id="蒙特卡洛强化学习-Monte-Carlo-Reinforcement-Learning"><a href="#蒙特卡洛强化学习-Monte-Carlo-Reinforcement-Learning" class="headerlink" title="蒙特卡洛强化学习 (Monte-Carlo Reinforcement Learning)"></a>蒙特卡洛强化学习 (Monte-Carlo Reinforcement Learning)</h2><h3 id="蒙特卡洛强化学习概念"><a href="#蒙特卡洛强化学习概念" class="headerlink" title="蒙特卡洛强化学习概念"></a>蒙特卡洛强化学习概念</h3><p>蒙特卡洛强化学习：是在不清楚MDP状态转移及即时奖励的情况下，直接从经历完整的<code>Episode</code>来学习状态价值，通常情况下某状态的价值等于在多个Episode中以该状态算得到的所有收获的平均。</p><blockquote><p>Episode ：agent根据某个策略执行一系列action到结束就是一个episode。</p></blockquote><p>注：<strong>收获</strong>不是针对Episode的，它存在于Episode内，针对于Episode中某一个状态。从这个状态开始经历完Episode时得到的有衰减的即时奖励的总和。从一个Episode中，我们可以得到该Episode内所有状态的收获。当一个状态在Episode内出现多次，该状态的收获有不同的计算方法，下文会讲到。</p><p><strong>完整的Episode</strong> 指必须从某一个状态开始，Agent与Environment交互直到<strong>终止状态</strong>，环境给出终止状态的即时收获为止。</p><h3 id="蒙特卡洛强化学习特点"><a href="#蒙特卡洛强化学习特点" class="headerlink" title="蒙特卡洛强化学习特点"></a>蒙特卡洛强化学习特点</h3><p>蒙特卡洛强化学习有如下特点：不基于模型本身，直接从经历过的Episode中学习，必须是<strong>完整的Episode</strong>，使用的思想就是用平均收获值代替价值。理论上Episode越多，结果越准确。</p><p>蒙特卡罗法通过采样若干经历完整的<code>状态序列(episode)</code>来估计状态的真实价值。所谓的经历完整，就是这个序列必须是达到终点的。比如下棋问题分出输赢，驾车问题成功到达终点或者失败。有了很多组这样经历完整的状态序列，我们就可以来近似的估计状态价值，进而求解预测和控制问题了。</p><h3 id="蒙特卡洛策略评估-Monte-Carlo-Policy-Evaluation"><a href="#蒙特卡洛策略评估-Monte-Carlo-Policy-Evaluation" class="headerlink" title="蒙特卡洛策略评估 (Monte-Carlo Policy Evaluation)"></a>蒙特卡洛策略评估 (Monte-Carlo Policy Evaluation)</h3><blockquote><p><strong>目标：</strong>在给定策略下，从一系列的完整Episode经历中学习,最后求得到该策略下的状态价值函数。</p></blockquote><p>在解决问题过程中主要使用的信息是一系列完整Episode。其包含的信息有：状态的转移、使用的行为序列、中间状态获得的即时奖励以及到达终止状态时获得的即时奖励。其特点是使用有限的、完整Episode产生的这些经验性信息经验性地推导出每个状态的平均收获，以此来替代收获的期望，而后者就是状态价值。通常需要掌握完整的MDP信息才能准确计算得到。</p><p>数学描述如下：</p><p>基于特定策略 $\pi$ 的一个Episode信息可以表示为如下的一个序列：<br>$$<br>\large S_1,A_1,R_2,…,S_k\backsim\pi<br>$$</p><p>$t$ 时刻，$S_t$ 的收获：<br>$$<br>\large G_t &#x3D; R_{t+1}+\gamma R_{t+2}+…+\gamma ^{T-1}R_T<br>$$</p><p>其中，$T$ 为终止时刻。</p><p>该策略下某一状态 $s$ 的价值：<br>$$<br>\large v_\pi(s) &#x3D; E_\pi[G_t \rvert S_t &#x3D; s]<br>$$</p><blockquote><p>注： $R_{t+1}$ 表示的是 $t$时刻agent在状态 $S_t$ 获得的即时奖励，下文都使用这种下标来表示即时奖励。更准确的表述为：个体在状态 $S_t$ 执行一个行为$a$ 后离开该状态获得的即时奖励。</p></blockquote><p>很多时候，即时奖励只出现在Episode结束状态时，但不能否认在中间状态也可能有即时奖励。公式里的 $R_t$ 指的是<strong>任何状态</strong>得到的即时奖励，这一点尤其要注意。</p><p>在状态转移过程中，可能发生一个状态经过一定的转移后又一次或多次返回该状态，此时在一个Episode里如何计算这个状态发生的次数和计算该Episode的收获呢？可以有如下两种方法：</p><h4 id="首次访问蒙特卡洛策略评估"><a href="#首次访问蒙特卡洛策略评估" class="headerlink" title="首次访问蒙特卡洛策略评估"></a>首次访问蒙特卡洛策略评估</h4><p>在给定一个策略，使用一系列完整Episode评估某一个状态s时，对于每一个Episode，仅当该状态<strong>第一次</strong>出现在一个 episode中时：</p><ul><li>状态出现的次数加1 ：$N(s) \leftarrow N(s) + 1$</li><li>总的收获更新：$S(s) \leftarrow S(s) + G_t$</li><li>状态 s 的价值：$V(s) &#x3D; S(s)&#x2F;N(s)$</li></ul><p>当 $N(s) \rightarrow \infty$ 时，$V(s)\rightarrow v_\pi(s)$</p><h4 id="每次访问蒙特卡洛策略评估"><a href="#每次访问蒙特卡洛策略评估" class="headerlink" title="每次访问蒙特卡洛策略评估"></a>每次访问蒙特卡洛策略评估</h4><p>在给定一个策略，使用一系列完整Episode评估某一个状态s时，对于每一个Episode，状态 s <strong>每次</strong>出现在一个epospde中时：</p><ul><li>状态出现的次数加1 ：$N(s) \leftarrow N(s) + 1$</li><li>总的收获更新：$S(s) \leftarrow S(s) + G_t$</li><li>状态 s 的价值：$V(s) &#x3D; S(s)&#x2F;N(s)$</li></ul><p>当 $N(s) \rightarrow \infty$ 时，$V(s)\rightarrow v_\pi(s)$。计算的公式与 <strong>首次访问蒙特卡洛策略评估</strong> 的公式相同，但是具体的意义却不同，下一以一个简单的例子进行说明。</p><p><strong>二十一点</strong> 二十一点又名黑杰克（Blackjack），是一种流行于赌场的游戏，其目标是使得你的扑克牌点数之和不超过21的情况下越大越好。K、Q、J和10牌都算作10点（一般记作T，即ten之意）；A 牌（ace）既可算作1点也可算作11点，由玩家自己决定（当玩家停牌时，点数一律视为最大而尽量不爆，如A+9为20，A+4+8为13，A+3+A视为15）。游戏开始时，会给玩家和庄家各发两张牌。庄家的牌一张正面朝上，一张背面朝上，玩家两张都是明牌（都正面朝上）如果玩家的两张牌分别是一张A，一张10点（可能是10，J， Q，K），这种情况称为<strong>天和</strong>，玩家直接获胜。除非庄家也是天和，那就是平局。如果玩家不是天和，那么他可以一张一张地继续要牌，直到他主动停止（停牌）或者牌的点数和超过21点（爆牌）。如果玩家选择停牌，就轮到庄家行动。庄家根据一个固定的策略进行游戏：他一直要牌，直到点数等于或超过17时停牌。如果庄家爆牌，那么玩家获胜，否则根据谁的点数更靠近21决定胜负或者平局。</p><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/blackj.jpg"></p><p>根据以上游戏规则，我们得到如下信息：</p><p><strong>状态空间：</strong> （多达200种，根据对状态的定义可以有不同的状态空间，这里采用的定义是牌的分数，不包括牌型）</p><ul><li>当前牌的分数（12 - 21），低于12时，你可以安全的再叫牌，所以没意义。</li><li>庄家出示的牌（A - 10），庄家会显示一张牌面给玩家，另一张为暗牌。</li><li>我有“useable” ace吗？（是或否）A既可以当1点也可以当11点。</li></ul><p><strong>行为空间：</strong></p><ul><li>停止要牌 stick</li><li>继续要牌 twist</li></ul><p><strong>奖励（停止要牌）：</strong></p><ul><li>+1：如果你的牌分数大于庄家分数</li><li>0： 如果两者分数相同</li><li>-1：如果你的牌分数小于庄家分数</li></ul><p><strong>奖励（继续要牌）：</strong></p><ul><li>-1：如果牌的分数&gt;21，并且进入终止状态</li><li>0：其它情况</li></ul><p><strong>状态转换（Transitions）</strong>：如果牌分小于12时，自动要牌</p><p><strong>当前策略</strong>：牌分只要小于20就继续要牌。</p><p><strong>求解问题：</strong>评估该策略的好坏。</p><p><strong>求解过程：</strong>使用庄家显示的牌面值、玩家当前牌面总分值来确定一个二维状态空间，区分手中有无A分别处理。统计每一牌局下决定状态的庄家和玩家牌面的状态数据，同时计算其最终收获。通过模拟多次牌局，计算每一个状态下的平均值，得到如下图示。</p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20201231132127235.png" alt="image-20201231132127235"></p><p><strong>最终结果：</strong>无论玩家手中是否有A牌，该策略在绝大多数情况下各状态价值都较低，只有在玩家拿到21分时状态价值有一个明显的提升。</p><p>这个例子只是使读者对蒙特卡洛策略评估方法有一个直观的认识。</p><p>为了尽可能使读者对MC方法有一个直接的认识，我们尝试模拟多个二十一点游戏牌局信息，假设我们仅研究初始状态下庄家一张明牌为4，玩家手中前两张牌和为15的情形，不考虑A牌。在给定策略下，玩家势必继续要牌，根据蒙特卡洛策略评估则可能会出现如下多种情形：</p><table><thead><tr><th>庄家总序列</th><th>玩家最终序列</th><th>玩家获得奖励</th><th>当前估计的状态价值</th></tr></thead><tbody><tr><td>4，10，3 （17点）</td><td>Q，5，5（20点）</td><td>+1</td><td>+1</td></tr><tr><td>4，J，7（21点）</td><td>9，6，8（23点，爆牌）</td><td>-1</td><td>0</td></tr><tr><td>4，10，2，8（24点，爆牌）</td><td>7，8，6（21点）</td><td>+1</td><td>0.333</td></tr><tr><td>4，4，2，7（17点）</td><td>J，5，Q（15点）</td><td>-1</td><td>0</td></tr><tr><td>4，9（13点）</td><td>5，K，4，8（27点，爆牌）</td><td>-1</td><td>-0.2</td></tr><tr><td>4，3，K（17点）</td><td>8，7，5（20点）</td><td>+1</td><td>0</td></tr><tr><td>…</td><td>…</td><td>…</td><td>…</td></tr></tbody></table><p>以上状态价值由 以下公式得出：</p><ul><li>状态出现的次数加1 ：$N(s) \leftarrow N(s) + 1$</li><li>总的收获更新：$S(s) \leftarrow S(s) + G_t$</li><li>状态 s 的价值：$V(s) &#x3D; S(s)&#x2F;N(s)$</li></ul><p>以前三个序列为例</p><ol><li>序列一： N &#x3D; 1， S &#x3D; 1，V &#x3D; 1 &#x2F; 1 &#x3D; 1;</li><li>序列二： N &#x3D; 2， S &#x3D; 1 - 1 &#x3D; 0，V &#x3D; 0 &#x2F; 2 &#x3D; 0;</li><li>序列三： N &#x3D; 3， S &#x3D; 0 + 1 &#x3D; 1，V &#x3D; 1 &#x2F; 3 &#x3D; 0.333</li></ol><p>可以看到，使用只有当牌不小于20的时候才停止叫牌这个策略（注：庄家不需要遵从这个策略），<strong>前6次平均价值为0</strong>，如果玩的牌局足够多，按照这样的方法可以针对每一个状态（庄家第一张明牌，玩家手中前两张牌分值合计）都可以制作这样一张表，进而计算玩家奖励的平均值。通过结果，可以发现这个策略并不能带来很高的玩家奖励。</p><p>这里给出表中第一个对局对应的信息序列（Episode）：<br>$$<br>\large S_0&lt;4,15&gt;, A_0&lt;要牌&gt;,R_1&lt;0&gt;,S_1&lt;4,20&gt;,A_1&lt;停止要牌&gt;，R_2&lt;+1&gt;<br>$$<br>可以看出，这个完整的Episode中包含两个状态，其中第一个状态的即时奖励为0，后一个状态是终止状态，根据规则，玩家赢得对局，获得终止状态的即时奖励+1。读者可以加深对即时奖励、完整Episode的理解。</p><p>通过上面的例子，我们使用蒙特卡洛方法求解的是<strong>平均收获</strong>，根据上面的例子，可以很清楚的知道这点。通常计算平均值就需要预先存储所有的数据，最后加和取平均，这样真的在计算机中计算时，会浪费很多空间。下面介绍一种更简单的方法——<strong>累进更新平均值（Incremental Mean）</strong></p><h3 id="累进更新平均值（Incremental-Mean）"><a href="#累进更新平均值（Incremental-Mean）" class="headerlink" title="累进更新平均值（Incremental Mean）"></a>累进更新平均值（Incremental Mean）</h3><p>这里提到了在实际操作时常用的一个实时更新均值的办法，使得在计算平均收获时不需要存储所有既往收获，而是每得到一次收获，就计算其平均收获。</p><p>理论公式如下：<br>$$<br>\large{<br>\begin{align}<br>\mu_k &#x3D;&amp; \frac{1}{k}\sum_{j&#x3D;1}^k x_j \<br>&#x3D;&amp; \frac{1}{k}\left(x_k + \sum_{j&#x3D;1}^{k-1} x_j \right) \<br>&#x3D;&amp; \frac{1}{k}(x_k+(k-1)\mu_{k-1}) \<br>&#x3D;&amp; \frac{1}{k}x_k+(1-\frac{1}{k})\mu_{k-1}\<br>&#x3D;&amp; \mu_{k-1} + \frac{1}{k}(x_k-\mu_{k-1})</p><p>\end{align}<br>}<br>$$<br>这个公式比较简单。把这个方法应用于蒙特卡洛策略评估，就得到下面的蒙特卡洛累进更新。</p><h3 id="蒙特卡洛累进更新"><a href="#蒙特卡洛累进更新" class="headerlink" title="蒙特卡洛累进更新"></a>蒙特卡洛累进更新</h3><p>对于一些列 Episodes中的每一个： $ S_1,A_1,R_2,…,S_k\backsim\pi $，对于Episode里的每一个状态$S_t$ ，有一个收获$G_t$，每碰到一次$S_t$ ，使用下面的公式计算状态的平均价值$V(S_t)$：<br>$$<br>\large V(S_t) \leftarrow V(S_t)  + \frac{1}{N(S_t)}(G_t -  V(S_t) )<br>$$<br>其中：<br>$$<br>\large N(S_t) &#x3D; N(S_t)  +1<br>$$<br>这样我们无论数据量是多还是少，算法需要的内存基本是固定的 。</p><p>有时候，尤其是海量数据做分布式迭代的时候，我们可能无法准确计算当前的次数$N(S_t)$，这时我们可以用一个系数$\alpha$来代替，即：<br>$$<br>\large V(S_t) &#x3D; V(S_t)  + \alpha(G_t -  V(S_t) )<br>$$</p><p>以上就是蒙特卡罗方法求解预测问题的整个过程。由于蒙特卡洛学习方法有许多缺点（后文会细说），因此实际应用并不多。接下来介绍实际常用的时序差分学习方法。</p><h2 id="时序差分学习-Temporal-Difference-Learning"><a href="#时序差分学习-Temporal-Difference-Learning" class="headerlink" title="时序差分学习 Temporal-Difference Learning"></a>时序差分学习 Temporal-Difference Learning</h2><h3 id="时序差分学习特点"><a href="#时序差分学习特点" class="headerlink" title="时序差分学习特点"></a>时序差分学习特点</h3><p>时序差分学习简称TD学习，它的特点如下：和蒙特卡洛学习一样，它也从Episode学习，不需要了解模型本身；但是它可以学习<strong>不完整</strong>的Episode，通过自身的引导（bootstrapping），猜测Episode的结果，同时持续更新这个猜测。</p><h3 id="时序差分策略评估"><a href="#时序差分策略评估" class="headerlink" title="时序差分策略评估"></a>时序差分策略评估</h3><p>我们已经学过，在Monte-Carlo学习中，使用实际的收获（return）$G_t$来更新价值（Value）：<br>$$<br>\large V(S_t) &#x3D; V(S_t)  + \alpha(G_t -  V(S_t) ) \tag1<br>$$<br>由第一章的收获公式(注：收获&#x2F;回报都是$G_t$，以后两者不会特别区分)可得：<br>$$<br>\large<br>G_t &#x3D; R_{t+1} + \gamma R_{t+2}+\gamma^2 R_{t+3} \ …\ +\gamma^{T-1}R_T &#x3D; \sum_{i&#x3D;0}^{\infty}\gamma^i R_{t+i+1} \quad \quad \gamma\in[0,1],\quad (T\rightarrow\infty)<br>$$</p><blockquote><p>注： $T$ 用来指示 $R$ 的下标，表明是第几个 $R$</p></blockquote><p>如果用$G_t$ 来更新价值的话，就可以写成如下：<br>$$<br>\large<br>G_t &#x3D; R_{t+1} + \gamma V{(S_{t+1})} \tag2<br>$$<br>在TD学习中，算法在估计某一个状态的价值时，用的是离开该状态的即刻奖励 $R_{t+1}$ 与下一个状态 $S_{t+1}$的预估价值乘以衰减系数$\gamma$ 组成，这符合Bellman方程的描述，故把（2）式带入（1）式得：<br>$$<br>\large V(S_t) &#x3D; V(S_t)  + \alpha(R_{t+1} + \gamma V{(S_{t+1})} -  V(S_t) ) \tag3<br>$$</p><p>其中（3）式中：</p><ul><li>$R_{t+1}+\gamma V(S_{t + 1})$  称为<strong>TD的目标值</strong></li><li>$\delta_t &#x3D; R_{t+1}+\gamma V(S_{t + 1})-V(S_t)$ 称为<strong>TD误差</strong></li></ul><p><strong>BootStrapping</strong> 指的就是TD目标值 $R_{t+1}+\gamma V(S_{t + 1})$ 代替收获$G_t$的过程，暂时把它翻译成“<strong>引导”</strong>。</p><h3 id="蒙特卡洛和TD策略评估差别"><a href="#蒙特卡洛和TD策略评估差别" class="headerlink" title="蒙特卡洛和TD策略评估差别"></a>蒙特卡洛和TD策略评估差别</h3><p><strong>例子——驾车返家：</strong>想象一下你下班后开车回家，需要预估整个行程花费的时间。假如一个人在驾车回家的路上突然碰到险情：对面迎来一辆车感觉要和你相撞，严重的话他可能面临死亡威胁，但是最后双方都采取了措施没有实际发生碰撞。如果使用蒙特卡洛学习，路上发生的这一险情可能引发的负向奖励不会被考虑进去，不会影响总的预测耗时；但是在TD学习时，碰到这样的险情，这个人会立即更新这个状态的价值，随后会发现这比之前的状态要糟糕，会立即考虑决策降低速度赢得时间，也就是说你不必像蒙特卡洛学习那样直到他死亡后才更新状态价值，那种情况下也无法更新状态价值。</p><p>TD算法相当于在整个返家的过程中（一个Episode），根据已经消耗的时间和预期还需要的时间来不断更新最终回家需要消耗的时间。</p><table><thead><tr><th>状态</th><th>已消耗时间（分钟）</th><th>预计仍需耗时（分钟）</th><th>预计总耗时（分钟）</th></tr></thead><tbody><tr><td>离开办公室</td><td>0</td><td>30</td><td>30</td></tr><tr><td>取车，发现下雨</td><td>5</td><td>35</td><td>40</td></tr><tr><td>离开高速公路</td><td>20</td><td>15</td><td>35</td></tr><tr><td>被迫跟在卡车后面</td><td>30</td><td>10</td><td>40</td></tr><tr><td>到达家所在街区</td><td>40</td><td>3</td><td>43</td></tr><tr><td>进入家门</td><td>43</td><td>0</td><td>43</td></tr></tbody></table><p>基于上表所示的数据，下图展示了蒙特卡洛学习和TD学习两种不同的学习策略来<strong>更新</strong>价值函数（各个状态的价值）。这里使用的是<strong>从某个状态预估的到家还需耗时</strong>来<strong>间接</strong>反映某状态的价值：某位置预估的到家时间越长，该位置价值越低，在优化决策时需要避免进入该状态。对于蒙特卡洛学习过程，驾驶员在路面上碰到各种情况时，他不会更新对于回家的预估时间，等他回到家得到了真实回家耗时后，他会重新估计在返家的路上着每一个主要节点状态到家的时间，在下一次返家的时候用新估计的时间来帮助决策；而对于TD学习，在一开始离开办公室的时候你可能会预估总耗时30分钟，但是当你取到车发现下雨的时候，你会立刻想到原来的预计过于乐观，因为既往的经验告诉你下雨会延长你的返家总时间，此时你会更新目前的状态价值估计，从原来的30分钟提高到40分钟。同样当你驾车离开高速公路时，会一路根据当前的状态（位置、路况等）对应的预估返家剩余时间，直到返回家门得到实际的返家总耗时。这一过程中，你会根据状态的变化实时更新该状态的价值。</p><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20210101171811202.png" alt="image-20210101171811202"></p><h4 id="MC-vs-TD-一"><a href="#MC-vs-TD-一" class="headerlink" title="MC vs. TD (一)"></a>MC vs. TD (一)</h4><p>通过上面的例子我们可以分析得出MC和TD的不同如下</p><ul><li>TD 在知道结果之前可以学习，MC必须等到最后结果才能学习</li><li>TD 可以在没有结果时学习，可以在持续进行的环境里学习, MC必须要等到完整的Episdo结束后才能学习</li></ul><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20210101173034838.png" alt="image-20210101173034838"></p><h4 id="MC-vs-TD-二"><a href="#MC-vs-TD-二" class="headerlink" title="MC vs. TD (二)"></a>MC vs. TD (二)</h4><p>$G_t$ : 实际收获，是基于某一策略状态价值的<strong>无偏</strong>估计：<br>$$<br>\large<br>G_t &#x3D; R_{t+1} + \gamma R_{t+2}+\gamma^2 R_{t+3} \ …\ +\gamma^{T-1}R_T<br>$$<br><code>TD target</code> ：TD目标值，是基于下一状态<strong>预估价值</strong>计算当前预估收获，是当前状态实际价值的<strong>有偏</strong>估计：<br>$$<br>\large  R_{t+1} + \gamma V{(S_{t+1})}<br>$$<br><code>True TD target</code>： 真实TD目标值，是基于下一状态的<strong>实际价值</strong>对当前状态实际价值的<strong>无偏</strong>估计：<br>$$<br>\large  R_{t+1} + \gamma v_\pi{(S_{t+1})}<br>$$<br><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20210101185956942.png" alt="image-20210101185956942"></p><ul><li>MC 没有偏倚（bias），但有着较高的变异性（Variance），且对初始值不敏感</li><li>TD 低变异性（Variance）, 但有一定程度的偏倚（bias），对初始值较敏感，通常比 MC 更高效</li></ul><p>这里的<strong>偏倚</strong>指的是距离期望的距离，预估的平均值与实际平均值的偏离程度；<strong>变异性</strong>指的是方差，评估单次采样结果相对于与平均值变动的范围大小。基本就是统计学上均值与方差的概念。</p><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20210101190031371.png" alt="image-20210101190031371"></p><p>对于MC和TD的区别，还可以用下面的例子来加深理解：</p><p><strong>例子——随机行走：</strong></p><p><strong>状态空间</strong>：如下图：A、B、C、D、E为中间状态，C作为起始状态。灰色方格表示终止状态</p><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20210101190514260.png" alt="image-20210101190514260"></p><p><strong>行为空间</strong>：除终止状态外，任一状态可以选择向左、向右两个行为之一</p><p><strong>即时奖励：</strong>右侧的终止状态得到即时奖励为1，左侧终止状态得到的即时奖励为0，在其他状态间转化得到的即时奖励是0</p><p><strong>状态转移</strong>：100%按行为进行状态转移，进入终止状态即终止</p><p><strong>衰减系数：</strong>1</p><p><strong>给定的策略</strong>：随机选择向左、向右两个行为</p><p><strong>问题：</strong>对这个MDP问题进行预测，也就是评估随机行走这个策略的价值，也就是计算该策略下每个状态的价值，也就是确定该MDP问题的状态价值函数</p><p><strong>求解：</strong>下图是使用TD算法得到的结果。横坐标显示的是状态，纵坐标是各状态的价值估计，一共5条折线，数字表明的是实际经历的Episode数量，true value所指的那根折线反映的是各状态的实际价值。第0次时，各状态的价值被初始化为0.5，经过1次、10次、100次后得到的价值函数越来越接近实际状态价值函数。</p><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20210101191128639.png" alt="image-20210101191128639"></p><p>下图比较了MC和TD算法的<strong>效率</strong>。横坐标是经历的Episode数量，纵坐标是计算得到的状态函数和实际状态函数下各状态价值的均方差。黑色是MC算法在不同step-size下的学习曲线，灰色的曲线使用TD算法。可以看出TD较MC更高效。此图还可以看出当step-size不是非常小的情况下，TD有可能得不到最终的实际价值，将会在某一区间震荡。</p><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20210101192751452.png" alt="image-20210101192751452"></p><p><strong>例子——AB：</strong></p><p><strong>已知：</strong>现有两个状态(A和B)，MDP未知，衰减系数为1，有如下表所示8个完整Episode的经验及对应的即时奖励，其中除了第1个Episode有状态转移外，其余7个均只有一个状态。</p><table><thead><tr><th>Episode</th><th>状态转移及奖励</th></tr></thead><tbody><tr><td>1</td><td>A：0，B：0</td></tr><tr><td>2</td><td>B：1</td></tr><tr><td>3</td><td>B：1</td></tr><tr><td>4</td><td>B：1</td></tr><tr><td>5</td><td>B：1</td></tr><tr><td>6</td><td>B：1</td></tr><tr><td>7</td><td>B：1</td></tr><tr><td>8</td><td>B：1</td></tr></tbody></table><p><strong>问题：依据仅有的Episode，衰减系数为1（即无衰减），计算状态A，B的价值分别是多少，即</strong>V(A)&#x3D;？， V(B)&#x3D;？</p><p><strong>答案：</strong>V(B) &#x3D; 6&#x2F;8，V(A)根据不同算法结果不同，用MC算法结果为0，TD则得出6&#x2F;8。</p><p><strong>解释：</strong></p><p>应用MC算法，由于需要完整的Episode，因此仅Episode1可以用来计算A的状态价值，很明显是0；同时B的价值是6&#x2F;8。</p><p>应用TD算法时，TD算法试图利用现有的Episode经验构建一个MDP（如下图），由于存在一个Episode使得状态A有后继状态B，因此状态A的价值是通过状态B的价值来计算的，同时经验表明A到B的转移概率是100%，且A状态的即时奖励是0，并且没有衰减，因此A的状态价值等于B的状态价值。</p><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20210102164122980.png" alt="image-20210102164122980"></p><p>MC算法试图收敛至一个能够<strong>最小化状态价值</strong>与<strong>实际收获</strong>的均方差的解决方案，这一均方差用公式表示为：<br>$$<br>\large \sum_{k&#x3D;1}^K \sum_{t&#x3D;1}^{T_k}(G_t^k-V(s_t^k))^2<br>$$<br>上式中，$k$ 表示的是Episode序号，$K$为总的Episode数量，$t$ 为一个Episode 内状态序号（第1，2，3，4…个状态等），$T_k$ 表示的是第$k$ 个Episode中总的状态数，$G_t^k$ 表示第$k$ 个Episode里t时刻状态$s_t$ 获得的最终收获，$V(s_t^k)$ 表示的是第 $k$ 个Episode里算法估计的 $t$时刻状态$s_t$的价值。</p><p>在例子<strong>AB</strong>中，利用MC算法，V(A)&#x3D;0。</p><p>TD算法则收敛至一个根据已有经验构建的最大可能的马儿可夫模型（MDP&lt;$S,A,P,R,\gamma$&gt;）的状态价值，也就是说TD算法将首先根据已有经验估计<strong>状态间的转移概率</strong>：<br>$$<br>\large P_{ss’}^a &#x3D; \frac{1}{N(s,a)}\sum_{k&#x3D;1}^K\sum_{t&#x3D;1}^{T_k}1(s_t^k,a_t^k,s_{t+1}^k &#x3D; s,a,s’)<br>$$<br>同时估计某一状态的<strong>即时奖励：</strong><br>$$<br>\large P_s^a &#x3D; \frac{1}{N(s,a)}\sum_{k&#x3D;1}^K\sum_{t&#x3D;1}^{T_k}1(s_t^k,a_t^k &#x3D; s,a)r_t^k<br>$$</p><p>最后计算该MDP的状态函数。</p><p>在例子<strong>AB</strong>中，利用TD算法，V(A)&#x3D;6&#x2F;8。</p><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20210102172416301.png" alt="image-20210102172416301"></p><blockquote><p>注：公式符号有所不同，但是表达意义一致。</p></blockquote><h4 id="MC-vs-TD-三"><a href="#MC-vs-TD-三" class="headerlink" title="MC vs. TD (三)"></a>MC vs. TD (三)</h4><p>通过比较可以看出，TD算法使用了MDP问题的马儿可夫性，在Markov 环境下更有效；但是MC算法并不利用马儿可夫性，通常在非Markov环境下更有效。</p><h2 id="DP，MC，TD-总结"><a href="#DP，MC，TD-总结" class="headerlink" title="DP，MC，TD 总结"></a>DP，MC，TD 总结</h2><p>Monte-Carlo，Temporal-Difference 和 Dynamic Programming 都是<strong>计算状态价值</strong>的一种方法，区别在于，前两种是在不知道Model的情况下的常用方法，这其中又以MC方法需要一个<strong>完整</strong>的Episode来更新状态价值，TD则不需要完整的Episode；DP方法则是基于Model（知道模型的运作方式）的计算状态价值的方法，它通过计算一个状态S所有可能的转移状态 S’ 及其转移概率以及对应的即时奖励来计算这个状态S的价值。</p><p><strong>关于是否Bootstrap：</strong>MC 没有引导数据，只使用实际收获；DP和TD都有引导数据。</p><p><strong>关于是否用样本来计算：</strong> MC和TD都是应用样本来估计实际的价值函数；而DP则是利用<strong>模型</strong>直接计算得到实际价值函数，没有样本或采样之说。</p><p>下面的几张图直观地体现了三种算法的区别：</p><ol><li>MC：采样，一次完整经历，用实际收获更新状态预估价值。</li></ol><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20210102180703567.png" alt="image-20210102180703567"></p><ol start="2"><li>TD：采样，经历可不完整，用喜爱状态（已有的状态）的预估状态价值预估收获，再更新预估价值。</li></ol><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20210102181120279.png" alt="image-20210102181120279"></p><ol start="3"><li>DP：没有采样，根据完整模型，依靠预估数据更新状态价值。</li></ol><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20210102181252019.png" alt="image-20210102181252019"></p><p>下图从两个维度解释了四种算法的差别，多了一个穷举法。这两个维度分别是：<strong>采样深度和广度</strong>。当使用单个采样，同时不走完整个Episode就是TD；当使用单个采样但走完整个Episode就是MC；当考虑全部样本可能性，但对每一个样本并不走完整个Episode时，就是DP；当既考虑所有Episode又把Episode从开始到终止遍历完，就变成了穷举法。</p><p>需要提及的是：DP利用的是整个MDP问题的模型，也就是状态转移概率，虽然它并不实际利用样本，但是它利用了整个模型的规律，因此认为是Full Width的。</p><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20210102181531691.png" alt="image-20210102181531691"></p><h2 id="λ时序差分强化学习TD-λ"><a href="#λ时序差分强化学习TD-λ" class="headerlink" title="λ时序差分强化学习TD(λ)"></a>λ时序差分强化学习TD(λ)</h2><p>先前所介绍的TD算法实际上都是TD(0)算法，括号内的数字0表示的是在当前状态下往前多看1步，要是往前多看2步更新状态价值会怎样？这就引入了<code>n-step</code>的概念。</p><h3 id="n-步预测-n-Step-Prediction"><a href="#n-步预测-n-Step-Prediction" class="headerlink" title="n-步预测 (n-Step Prediction)"></a>n-步预测 (n-Step Prediction)</h3><p>在当前状态往前行动n步，计算n步的return，同样TD target 也由2部分组成，已走的步数使用确定的即时reward，剩下的使用估计的状态价值替代。</p><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20210103181438607.png" alt="image-20210103181438607"></p><blockquote><p>注：图中空心大圆圈表示状态，实心小圆圈表示行为</p></blockquote><h3 id="n-步收获-n-Step-Return"><a href="#n-步收获-n-Step-Return" class="headerlink" title="n-步收获(n-Step Return  )"></a>n-步收获(n-Step Return  )</h3><p>TD或TD(0)是基于<code>1-步</code>预测的，MC则是基于<code>∞-步</code>预测的。</p><table><thead><tr><th>n&#x3D;1</th><th>TD 或TD(0)</th><th>$G_t^{(1)} &#x3D; R_{t+1} + \gamma(S_{t+1}) $</th></tr></thead><tbody><tr><td>n&#x3D;2</td><td></td><td>$G_t^{(2)} &#x3D; R_{t+1} + \gamma R_{t+2}+\gamma^2 V(S_{t+2})$</td></tr><tr><td>…</td><td></td><td></td></tr><tr><td>n&#x3D; ∞</td><td>MC</td><td>$G_t^\infin &#x3D; R_{t+1} + \gamma R_{t+2}+\gamma^2 R_{t+3} \ …\ +\gamma^{T-1}R_T$</td></tr></tbody></table><blockquote><p>注： n &#x3D; 2时不写成TD(2)</p></blockquote><p><strong>n-步收获：</strong><br>$$<br>\large G_t^\infin &#x3D; R_{t+1} + \gamma R_{t+2}+\gamma^2 R_{t+3} \ …\ +\gamma^{n-1}R_{t+n} + \gamma^nV(S_{t+n})<br>$$<br>那么，n步TD学习状态<strong>价值函数</strong>的更新公式为：<br>$$<br>\large V(S_t) \leftarrow V(S_t)  + \alpha(G_t^a -  V(S_t) )<br>$$<br>既然存在n-步预测，那么n&#x3D;？时效果最好呢，下面的例子试图回答这个问题：</p><p><strong>例子：大规模随机行走</strong></p><p>这个示例研究了使用多个不同步数预测联合不同步长(step-size，公式里的系数α）时，分别在在线和离线状态时状态函数均方差的差别。所有研究使用了10个Episode。离线与在线的区别在于，离线是在经历所有10个Episode后进行状态价值更新；而在线则至多经历一个Episode就更新依次状态价值。</p><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20210105123410790.png" alt="image-20210105123410790"></p><p>结果如图表明，离线和在线之间曲线的形态差别不明显；从步数上来看，步数越长，越接近MC算法，均方差越大。对于这个大规模随机行走示例，在线计算比较好的步数是3-5步，离线计算比较好的是6-8步。但是不同的问题其对应的比较高效的步数不是一成不变的。因此选择多少步数作为一个较优的计算参数也是一个问题。</p><p>这里我们引入了一个新的参数：λ。通过引入这个新的参数，可以做到在不增加计算复杂度的情况下综合考虑所有步数的预测。这就是<strong>λ预测</strong>和<strong>λ收获。</strong></p><h3 id="λ收获"><a href="#λ收获" class="headerlink" title="λ收获"></a>λ收获</h3><p>λ-收获$G_t^\lambda$综合考虑了从1到∞的所有收获，它给其中的任意一个<code>n-step</code> 收获施加一定的权重$(1-\lambda)\lambda^{n-1}$。通过这样的权重设计，得到如下的公式：<br>$$<br>\large G_t^\lambda &#x3D;(1-\lambda)\sum_{n&#x3D;1}^\infin\lambda^{n-1}G_t^{(n)}<br>$$</p><h3 id="λ预测"><a href="#λ预测" class="headerlink" title="λ预测"></a>λ预测</h3><p>对应的λ-预测写成TD(λ):<br>$$<br>\large V(S_t) \leftarrow V(S_t) + \alpha(G_t^\lambda-V(S_t))<br>$$<br>下图是各步收获的<strong>权重分配图</strong>，图中最后一列λ的指数是$T-t-1$ 。$T$ 为终止状态的时刻步数，$t$ 为当前状态的时刻步数，所有的权重加起来为1。</p><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20210105130732141.png" alt="image-20210105130732141"></p><h3 id="TD-λ-对于权重分配的图解"><a href="#TD-λ-对于权重分配的图解" class="headerlink" title="TD(λ)对于权重分配的图解"></a>TD(λ)对于权重分配的图解</h3><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20210105130951563.png" alt="image-20210105130951563"></p><p>这张图还是比较好理解，例如对于n&#x3D;3的3-步收获，赋予其在 $\lambda$ 收获中的权重如左侧阴影部分面积，对于终止状态的T-步收获，T以后的<strong>所有</strong>阴影部分面积。而所有节段面积之和为1。这种几何级数的设计也考虑了算法实现的计算方便性。</p><p>TD((λ)的设计使得Episode中，<strong>后一个状态的状态价值与之前所有状态的状态价值有关，同时也可以说成是一个状态价值参与决定了后续所有状态的状态价值。但是每个状态的价值对于后续状态价值的影响权重是不同的</strong>。我们可以从两个方向来理解TD(λ)：</p><p><strong>前向认识TD(λ)</strong></p><p>引入了λ之后，会发现要更新一个状态的状态价值，必须要走完整个Episode获得每一个状态的即时奖励以及最终状态获得的即时奖励。这和MC算法的要求一样，因此TD(λ)算法有着和MC方法一样的劣势。λ取值区间为[0,1]，<strong>当λ&#x3D;1时对应的就是MC算法</strong>。这个实际计算带来了不便。</p><p><strong>反向认识TD(λ)</strong></p><p>TD(λ)从另一方面提供了一个单步更新的机制，通过下面的示例来说明。</p><p><strong>示例——被电击的原因</strong></p><p>这是之前见过的一个例子，老鼠在连续接受了3次响铃和1次亮灯信号后遭到了电击，那么在分析遭电击的原因时，到底是响铃的因素较重要还是亮灯的因素更重要呢？</p><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20210105131812554.png" alt="image-20210105131812554"></p><p>两个概念：</p><p><strong>频率启发 (Frequency heuristic)：</strong>将原因归因于出现频率最高的状态</p><p><strong>就近启发 (Recency heuristic)：</strong>将原因归因于较近的几次状态</p><p>给每一个状态引入一个数值：<strong>效用追踪</strong>（<strong>Eligibility Traces, ES，也有翻译成“资质追踪”，这是同一个概念从两个不同的角度理解得到的不同翻译</strong>），可以结合上述两个启发。定义：</p><p>$$<br>E_0(s) &#x3D; 0 \<br>E_t(s) &#x3D; \gamma\lambda E_{t-1}(s)+1(S_t &#x3D; s)<br>$$<br>其中 $1(S_t &#x3D; s)$ 是一个条件判断表达式。</p><p>下图给出了$E_t(s)$ 关于$t$的一个可能的曲线图：</p><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20210105152337864.png" alt="image-20210105152337864"></p><p>该图横坐标是时间$t$，横坐标下有竖线的位置代表当前进入了状态 $s$ ，纵坐标是效用追踪值 $E$。可以看出当某一状态连续出现，E值会在一定衰减的基础上有一个单位数值的提高，此时将增加该状态对于最终收获贡献的比重，因而在更新该状态价值的时候可以较多地考虑最终收获的影响。同时如果该状态距离最终状态较远，则其对最终收获的贡献越小，在更新该状态时也不需要太多的考虑最终收获。</p><p>特别的$E$值并不需要等到完整的Episode结束才能计算出来，它可以每经过一个时刻就得到更新。$E$值存在<strong>饱和现象</strong>，有一个瞬间最高上限：<br>$$<br>\large E_{max} &#x3D; 1&#x2F;(1-\gamma\lambda)<br>$$<br>把刚才的描述体现在公式里<strong>更新状态价值</strong>，得到如下式子：<br>$$<br>\large \delta_t &#x3D; R_{t+1} +\gamma V(S_{t+1})-V(S_t)<br>$$</p><p>$$<br>\large V(s)\leftarrow V(s)+\alpha\delta_tE_t(s)<br>$$</p><blockquote><p>注：每一个状态都有一个 $E$值， $E$值随时间而变化。</p></blockquote><p>当λ&#x3D;0时，只有当前状态得到更新，等同于TD(0)算法；</p><p>当λ&#x3D;1时，TD(1)粗略看与每次访问的MC算法等同；</p><ul><li>在线更新时，状态价值差每一步都会有积累；</li><li>离线更新时，TD(1)等同于MC算法。</li></ul><blockquote><p>注：ET是一个非常符合神经科学相关理论的、非常精巧的设计。把它看成是神经元的一个参数，它反映了神经元对某一刺激的敏感性和适应性。神经元在接受刺激时会有反馈，在持续刺激时反馈一般也比较强，当间歇一段时间不刺激时，神经元又逐渐趋于静息状态；同时不论如何增加刺激的频率，神经元有一个最大饱和反馈。</p></blockquote><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>下表给出了λ取各种值时，不同算法在不同情况下的关系。</p><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20210105155527593.png" alt="image-20210105155527593"></p><p>相较于MC算法，TD算法应用更广，是一个非常有用的强化学习方法，在下一讲讲解控制相关的算法时会详细介绍TD算法的实现。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://zhuanlan.zhihu.com/p/28107168">https://zhuanlan.zhihu.com/p/28107168</a></p><p><a href="https://www.cnblogs.com/pinard/p/9492980.html">https://www.cnblogs.com/pinard/p/9492980.html</a></p><p><a href="http://www.incompleteideas.net/book/the-book.html">http://www.incompleteideas.net/book/the-book.html</a></p><p><a href="https://www.davidsilver.uk/wp-content/uploads/2020/03/MC-TD.pdf">https://www.davidsilver.uk/wp-content/uploads/2020/03/MC-TD.pdf</a></p><p><a href="https://baike.baidu.com/item/21%E7%82%B9/5481683?fr=aladdin">https://baike.baidu.com/item/21点/5481683?fr=aladdin</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;强化学习—免模型预测&quot;&gt;&lt;a href=&quot;#强化学习—免模型预测&quot; class=&quot;headerlink&quot; title=&quot;强化学习—免模型预测&quot;&gt;&lt;/a&gt;强化学习—免模型预测&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;作者：YJLAugus  博客： &lt;a href=</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://zhaohongqiangsoliva.github.io/public/2022/08/16/2021-10-02-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-03-MCMC/"/>
    <id>http://zhaohongqiangsoliva.github.io/public/2022/08/16/2021-10-02-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-03-MCMC/</id>
    <published>2022-08-16T09:18:55.588Z</published>
    <updated>2022-08-16T00:56:05.185Z</updated>
    
    <content type="html"><![CDATA[<h1 id="蒙特卡洛（Markov-Chain-amp-Monte-Carlo-MCMC）方法"><a href="#蒙特卡洛（Markov-Chain-amp-Monte-Carlo-MCMC）方法" class="headerlink" title="蒙特卡洛（Markov Chain &amp; Monte Carlo, MCMC）方法"></a>蒙特卡洛（Markov Chain &amp; Monte Carlo, MCMC）方法</h1><blockquote><p>作者：YJLAugus  博客： <a href="https://www.cnblogs.com/yjlaugus">https://www.cnblogs.com/yjlaugus</a> 项目地址：<a href="https://github.com/YJLAugus/Reinforcement-Learning-Notes%EF%BC%8C%E5%A6%82%E6%9E%9C%E6%84%9F%E8%A7%89%E5%AF%B9%E6%82%A8%E6%9C%89%E6%89%80%E5%B8%AE%E5%8A%A9%EF%BC%8C%E7%83%A6%E8%AF%B7%E7%82%B9%E4%B8%AA%E2%AD%90Star%E3%80%82">https://github.com/YJLAugus/Reinforcement-Learning-Notes，如果感觉对您有所帮助，烦请点个⭐Star。</a></p></blockquote><h2 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h2><p>20世纪40年代，在John von Neumann，Stanislaw Ulam 和 Nicholas Metropolis 在洛斯阿拉莫斯国家实验室为核武器计划工作时，发明了蒙特卡罗方法。因为Ulam的叔叔经常在蒙特卡洛赌场输钱得名，而蒙特卡罗方法正是以概率为基础的方法。</p><p>蒙特卡洛是摩纳哥的一个小城，蒙特卡洛是<a href="https://baike.baidu.com/item/%E6%91%A9%E7%BA%B3%E5%93%A5/127488">摩纳哥</a>公国的一座城市，位于欧洲<a href="https://baike.baidu.com/item/%E5%9C%B0%E4%B8%AD%E6%B5%B7/11515">地中海</a>之滨、<a href="https://baike.baidu.com/item/%E6%B3%95%E5%9B%BD/1173384">法国</a>的东南方，属于一个版图很小的国家<a href="https://baike.baidu.com/item/%E6%91%A9%E7%BA%B3%E5%93%A5%E5%85%AC%E5%9B%BD/4428850">摩纳哥公国</a>，世人称之为“赌博之国”、“袖珍之国”、“邮票小国” ， 很漂亮的一座小城。</p><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/MonteCarlo.jpg"></p><h2 id="算法介绍"><a href="#算法介绍" class="headerlink" title="算法介绍"></a>算法介绍</h2><ul><li><p>不是 “蒙特卡洛” 发明的，“蒙特卡洛”仅仅是一个城市的名字。由冯·诺依曼、乌拉姆等人发明。这是基于概率的方法的一个统称。</p></li><li><p>常于拉斯维加斯（Las Vegas）方法比较。两种方法各有侧重点：</p><p>蒙特卡洛（Monte Carlo）：民意调查——抽样，并估算到最优解。采样越多，越近似最优解。</p><p>拉斯维加斯（Las Vegas）：找人——必须要精确到找到那个人。采样越多，越有机会找到最优解。</p></li><li><p>相关算法：</p><p>蒙特卡洛算法、蒙特卡洛模拟、蒙特卡洛过程、蒙特卡洛搜索树（AlphaGo就是基于此算法）</p></li></ul><h3 id="工作原理"><a href="#工作原理" class="headerlink" title="工作原理"></a>工作原理</h3><p>不断抽样（近似采样），逐渐逼近最优解。</p><h3 id="为什么要采样（采样动机）？"><a href="#为什么要采样（采样动机）？" class="headerlink" title="为什么要采样（采样动机）？"></a>为什么要采样（采样动机）？</h3><ul><li>采样本身就是常见的任务：机器学习中，产生一堆样本，然后进行采样分析。</li><li><code>求和</code>或者是<code>求积分</code>的运算（比如下面的例子）。</li></ul><h3 id="假定已经采样完毕，那么什么是好的样本？"><a href="#假定已经采样完毕，那么什么是好的样本？" class="headerlink" title="假定已经采样完毕，那么什么是好的样本？"></a>假定已经采样完毕，那么什么是好的样本？</h3><ul><li>样本趋向于高概率区域（同时兼顾其他区域）：红色球区域</li><li>样本与样本之间相互独立： 也就是说 在下图红色球最密集区域的球不能有相互联系，不然依然是采样失败的，不能算好的样本。</li></ul><p>如下图的概率密度函数图所示，只有样本落在高概率的区域，越集中的样本才算是好的样本（红色球），相反的，绿色球样本就不算是好的样本</p><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/mtclpf.svg"></p><h3 id="一个例子"><a href="#一个例子" class="headerlink" title="一个例子"></a>一个例子</h3><p>利用蒙特卡洛方法计算圆周率<code>pi</code> 。如下图所示：</p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/pi.png" style="zoom: 67%;" /><p>从上面可得，扇形的面积$S_扇 &#x3D; \pi·1^2·1 ·\frac{1}{4}&#x3D;\frac{\pi}{4}$。正方形的面积为$S_方 &#x3D; 1$ ，可得：一个关系如下：<br>$$<br>\frac {扇形面积}{正方形面积} &#x3D; \frac{\pi}{4}<br>$$</p><p>接下来，如果在下面的图中随机<strong>打点</strong>，那么点落在绿色扇形区域的概率就是$P &#x3D; \frac {扇形面积}{正方形面积} &#x3D; \frac{\pi}{4}$ ，并最终得到 $\pi &#x3D; 4P$</p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20201208140817233.png" alt="image-20201208140817233"></p><p>在程序中实现我们的算法：可以发现，随着样本空间的增加，利用蒙特卡洛算法得到$\pi$ 的值越接近真实的$\pi$ 值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line">total = <span class="number">1000000</span></span><br><span class="line">in_count = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(total):</span><br><span class="line">    x = random.random()</span><br><span class="line">    y = random.random()</span><br><span class="line"></span><br><span class="line">    dis = (x ** <span class="number">2</span> + y ** <span class="number">2</span>) ** <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> dis &lt;= <span class="number">1</span>:</span><br><span class="line">        in_count += <span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Π是:&#x27;</span>, <span class="number">4</span>*in_count/total)</span><br><span class="line"></span><br><span class="line"><span class="comment"># PI = 概率 * 4</span></span><br><span class="line"><span class="comment"># 5       Π是: 4.0</span></span><br><span class="line"><span class="comment"># 100     Π是: 3.28</span></span><br><span class="line"><span class="comment"># 1000    Π是: 3.244</span></span><br><span class="line"><span class="comment"># 10000   Π是: 3.1524</span></span><br><span class="line"><span class="comment"># 100000  Π是: 3.15212</span></span><br><span class="line"><span class="comment"># 1000000 Π是: 3.141696</span></span><br></pre></td></tr></table></figure><h2 id="简单应用"><a href="#简单应用" class="headerlink" title="简单应用"></a>简单应用</h2><p>现在进行一个简单的应用，对于上面的规则形状，我们可以很方便的计算出图像的面积，但是对于不规则的图形，就不是那么容易求得了，这里就用蒙特卡洛方法进行计算不规则图形的面积。如下图，如果我们计算图中白色区域的面积，该如何去求得呢？</p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20201208192324680.png" alt="image-20201208192324680"></p><p>根据，蒙特卡洛方法，我们还是采用“打点”的方式，总点数为 <code>total_count</code> ，在白色区域的点数为<code>in_count</code>。那么点落在白色区域的概率就是 <code>in_count/total_count</code>，最后用这个概率乘以整张图的面积，就可以大概的估算出白色区域的面积，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span>  Image</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">img = Image.<span class="built_in">open</span>(<span class="string">&#x27;panda.jpg&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 获取像素的颜色</span></span><br><span class="line">total_count = <span class="number">1000000</span></span><br><span class="line">in_count = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(total_count):</span><br><span class="line">    x = random.randint(<span class="number">0</span>, img.width-<span class="number">1</span>)</span><br><span class="line">    y = random.randint(<span class="number">0</span>, img.height-<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># color = img.getpixel((x, y))</span></span><br><span class="line">    <span class="comment"># print(color)</span></span><br><span class="line">    <span class="keyword">if</span> img.getpixel((x, y)) == (<span class="number">255</span>, <span class="number">255</span>, <span class="number">255</span>):</span><br><span class="line">        in_count += <span class="number">1</span></span><br><span class="line">p = in_count/total_count</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">int</span>(img.width*img.height * p))</span><br><span class="line"><span class="comment"># 20        132240</span></span><br><span class="line"><span class="comment"># 100       143811</span></span><br><span class="line"><span class="comment"># 1000      131744</span></span><br><span class="line"><span class="comment"># 10000     130388</span></span><br><span class="line"><span class="comment"># 100000    130739</span></span><br><span class="line"><span class="comment"># 1000000   130699</span></span><br><span class="line"><span class="comment"># 1.图片读取</span></span><br></pre></td></tr></table></figure><p>接下来进行一个准确的遍历。也就是白色区域的真正的面积值，可以发现，和上面的几乎一致，利用蒙特卡洛算法得到的是 130699，准确的数据是130686， 相差无几。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># right: 130686</span></span><br><span class="line"><span class="comment"># 1.图片读取</span></span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span>  Image</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">img = Image.<span class="built_in">open</span>(<span class="string">&#x27;panda.jpg&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 获取像素的颜色</span></span><br><span class="line">count = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(img.width):</span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> <span class="built_in">range</span>(img.height):</span><br><span class="line">        <span class="keyword">if</span> (<span class="number">255</span>, <span class="number">255</span>, <span class="number">255</span>) == img.getpixel((x,y)):</span><br><span class="line">            count += <span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(count)</span><br></pre></td></tr></table></figure><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://www.bilibili.com/video/BV1Gs411g7EJ?t=1690">https://www.bilibili.com/video/BV1Gs411g7EJ?t=1690</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;蒙特卡洛（Markov-Chain-amp-Monte-Carlo-MCMC）方法&quot;&gt;&lt;a href=&quot;#蒙特卡洛（Markov-Chain-amp-Monte-Carlo-MCMC）方法&quot; class=&quot;headerlink&quot; title=&quot;蒙特卡洛（Marko</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://zhaohongqiangsoliva.github.io/public/2022/08/16/2021-10-02-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-02-DP/"/>
    <id>http://zhaohongqiangsoliva.github.io/public/2022/08/16/2021-10-02-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-02-DP/</id>
    <published>2022-08-16T09:18:55.586Z</published>
    <updated>2022-08-16T00:56:05.185Z</updated>
    
    <content type="html"><![CDATA[<h1 id="强化学习-动态规划-DP"><a href="#强化学习-动态规划-DP" class="headerlink" title="强化学习-动态规划-DP"></a>强化学习-动态规划-DP</h1><blockquote><p>作者：YJLAugus  博客： <a href="https://www.cnblogs.com/yjlaugus">https://www.cnblogs.com/yjlaugus</a> 项目地址：<a href="https://github.com/YJLAugus/Reinforcement-Learning-Notes%EF%BC%8C%E5%A6%82%E6%9E%9C%E6%84%9F%E8%A7%89%E5%AF%B9%E6%82%A8%E6%9C%89%E6%89%80%E5%B8%AE%E5%8A%A9%EF%BC%8C%E7%83%A6%E8%AF%B7%E7%82%B9%E4%B8%AA%E2%AD%90Star%E3%80%82">https://github.com/YJLAugus/Reinforcement-Learning-Notes，如果感觉对您有所帮助，烦请点个⭐Star。</a></p></blockquote><p>这里讲动态规划，主要是用动态规划来解决MDP的中最优的策略，并得出最优的价值函数。这节主要介绍两中动态规划的方法，一个是<strong>策略迭代</strong>方法，另一个是<strong>价值迭代</strong>方法。</p><h2 id="策略迭代"><a href="#策略迭代" class="headerlink" title="策略迭代"></a>策略迭代</h2><p>策略迭代主要分两个，一个是<strong>策略评估</strong>，一个是<strong>策略改进</strong>。定义比较简单，分别如下：</p><p><strong>策略评估</strong>：给定任意的一个 $\pi$ ，求出 状态价值函数 $v_\pi(s)$ 或者状态动作价值函数 $q_\pi(s,a)$。<br><strong>策略改进</strong>：依据策略评估得出来的 $v_\pi(s)$ 或者 $q_\pi(s,a)$，从其中构造出一个新的 $\pi’$ ，使得 $\pi’ \geq \pi $ ，这样就完成了一次迭代。</p><ul><li>换句话说，就是在每个状态下根据$v_\pi(s)$ 或者 $q_\pi(s,a)$选择一个最优的策略，这个策略就是 $\pi’$ 。</li><li>这种根据<code>原策略的价值函数</code>执行贪心算法，来构造一个更好策略的过程，我们称为策略改进。<br><strong>策略迭代</strong>： 就是递归以上两个过程，以上面的例子，得到$\pi’$的价值函数，然后再以$\pi’$的价值函数构造一个新的$\pi’’$ ，不断迭代。</li></ul><h3 id="策略评估-解析解"><a href="#策略评估-解析解" class="headerlink" title="策略评估-解析解"></a>策略评估-解析解</h3><p>策略评估，就是已知 MDP，“已知MDP”的意思是知道其动态特性，也就是 $p(s’,r \mid s,a)$，给定$\pi$ ，求$v_\pi(s)$。</p><p>已知$(\forall s\in S)$ ，即有多少个$s$ 就会有多少$v_\pi(s)$，可得列向量如下：<br>$$<br>\large<br>v_\pi(s)&#x3D; \begin{pmatrix}<br>v_\pi(s_1)\<br>v_\pi(s_2)\<br>\vdots    \<br>v_\pi(s_{\lvert S \rvert})</p><p>\end{pmatrix} _{\lvert S \rvert \times 1}<br>$$</p><p>上节课我们得到价值函数的公式，并进一步推导得到贝尔曼期望方程：<br>$$<br>\large<br>{\begin{align}<br>v_\pi(s) &#x3D;&amp; E_\pi[G_t \mid S_t &#x3D; s] \<br>&#x3D;&amp; E_\pi[R_{t+1} + \gamma v_\pi(S_{t+1} \mid S_t&#x3D;s)]\<br>&#x3D;&amp; v_\pi(s) &#x3D; \sum_{a\in A} \pi(a\mid s) \sum_{s’,r}p(s’,r \mid s,a)[r+ \gamma v_\pi(s’)]<br>\end{align}<br>}<br>$$<br>从上面的列向量中得知，如果我们想要求整个的$v_\pi(s)$ ，就是把$v_\pi(s_1),v_\pi(s_2),v_\pi(s_3)…$ 等所有的价值函数都要求出来。我们这节的目的就是把公式中的累加符号去掉，得到另一个更为简单的式子。接下来我们对下面的式子进行化简如下：<br>$$<br>\large{<br>\begin{align} </p><p>v_\pi(s) &#x3D;&amp; \sum_{a\in A} \pi(a\mid s) \sum_{s’,r}p(s’,r \mid s,a)[r+ \gamma v_\pi(s’)]\<br>&#x3D; &amp; \underbrace {\sum_{a\in A} \pi(a\mid s) \sum_{s’,r}p(s’,r \mid s,a) \cdot r }_{①}</p><p>+<br>\underbrace{\gamma \sum_{a\in A} \pi(a\mid s) \sum_{s’,r}p(s’,r \mid s,a) \cdot v_\pi(s’)}_{②}</p><p>\end{align}<br>}<br>$$<br><strong>分解①式</strong>：对于①式而言，$s’$ 只出现在函数$p(s’,r \mid s,a)$ 中，是可以积分积掉的。故由①得：<br>$$<br>\large{<br>\begin{align}</p><p>\sum_{a\in A} \pi(a\mid s) \sum_{s’,r}p(s’,r \mid s,a) \cdot r  \<br>&#x3D;&amp; \sum_{a\in A}  \pi(a\mid s) \underbrace {\sum_{r}  r\cdot p(r \mid s,a)}<em>{E_\pi[R</em>{t+1}  \mid S_t&#x3D;s,A_t&#x3D;a]} \quad\quad (1)  \</p><p>\end{align}<br>}<br>$$<br>（1）式正好是期望，此时我们也定义一个函数（记号） $r(s,a) \dot{&#x3D;} E_\pi[R_{t+1}  \mid S_t&#x3D;s,A_t&#x3D;a]$ 。表示$s,a$ 这个二元组的收益。故我们的推导变成如下：<br>$$<br>\large{<br>\begin{align}</p><p>\sum_{a\in A} \pi(a\mid s) \sum_{s’,r}p(s’,r \mid s,a) \cdot r  \<br>&#x3D;&amp; \sum_{a\in A} \pi(a\mid s) \underbrace {\sum_{r}  r\cdot p(r \mid s,a)}<em>{r(s,a) \dot{&#x3D;}E_\pi[R</em>{t+1}  \mid S_t&#x3D;s,A_t&#x3D;a]} \quad\quad (1)  \<br>&#x3D;&amp; \sum_{a\in A} \pi(a\mid s) \cdot r(s,a) \quad\quad\quad\quad \quad\quad \quad\quad\ (2)</p><p>\end{align}<br>}<br>$$<br>（2）式中，如果把 $a$ 都积分掉，那么（2）式就和$a$ 就没关系了，只和$s$ 有关，这里我们再次引入一个记号$r_\pi(s)$用来表示这个新的关系如下：<br>$$<br>\large{<br>\begin{align}</p><p>\sum_{a\in A} \pi(a\mid s) \sum_{s’,r}p(s’,r \mid s,a) \cdot r  \<br>&#x3D;&amp; \sum_{a\in A} \pi(a\mid s) \underbrace {\sum_{r}  r\cdot p(r \mid s,a)}<em>{r(s,a) \dot{&#x3D;}E_\pi[R</em>{t+1}  \mid S_t&#x3D;s,A_t&#x3D;a]} \quad\quad (1)  \<br>&#x3D;&amp; \sum_{a\in A} \pi(a\mid s) \cdot r(s,a) \quad\quad\quad\quad \quad\quad \quad\quad\ (2) \<br>\dot{&#x3D;}&amp;r_\pi(s)  \quad\quad\quad\quad \quad\quad \quad\quad\quad \quad\quad\ \quad\quad \quad\quad\ (3)</p><p>\end{align}<br>}<br>$$<br>此时，①式化简至此，需要提到的一点是，$r_\pi(s)$ 应该由多少个呢？在一开始我们就提到过 “$S$中有多少个$s$ 就会有多少$v_\pi(s)$，” ，显然 $r_\pi(s)$ 的数量也是 $\lvert S \rvert$ 个。故可得列向量如下:<br>$$<br>\large<br>r_\pi(s)&#x3D; \begin{pmatrix}<br>r_\pi(s_1)\<br>r_\pi(s_2)\<br>\vdots    \<br>r_\pi(s_{\lvert S \rvert})</p><p>\end{pmatrix} _{\lvert S \rvert \times 1}<br>$$<br><strong>分解②式</strong>：在 $p(s’,r \mid s,a) \cdot v_\pi(s’)$ 中，我们发现 $s’$ 在式子中都存在，但是$r$ 只存在于函数 $p(s’,r \mid s,a)$ 中，故积分可以积掉，则②式可以进一步写成：<br>$$<br>\large{<br>\begin{align}</p><p>\gamma \sum_{a\in A} \pi(a\mid s) \sum_{s’,r}p(s’,r \mid s,a) \cdot v_\pi(s’)<br>&#x3D;&amp; \gamma \sum_{a\in A} \pi(a\mid s) \sum_{s’}\underbrace{p(s’ \mid s,a)}_{状态转移函数} \cdot v_\pi(s’) \quad\quad (4)</p><p>\end{align}<br>}<br>$$<br>在（4）式中，可以发现在两个累加号中都有 $a$（注：虽然也都有 $s$，但是累加号下面是$a$ ，故积分只能积掉 $a$），积分可以积掉，但是一定和 $s,s’$ 有关，故引出一个记号 $P_\pi(s,s’)$ 来表示。 故继续推导如下：<br>$$<br>\large{<br>\begin{align}</p><p>\gamma \sum_{a\in A} \pi(a\mid s) \sum_{s’,r}p(s’,r \mid s,a) \cdot v_\pi(s’)<br>&#x3D;&amp; \gamma \sum_{a\in A} \pi(a\mid s) \sum_{s’}\underbrace{p(s’ \mid s,a)}_{状态转移函数} \cdot v_\pi(s’) \quad\quad (4) \</p><p>&#x3D;&amp; \gamma \sum_{s’} \underbrace{\sum_{a\in A} \pi(a\mid s) p(s’ \mid s,a)}_{P_\pi(s,s’)} \cdot v_\pi(s’) \quad\quad (5) \</p><p>&#x3D;&amp;  \gamma \sum_{s’} P_\pi(s,s’) \cdot v_\pi(s’) \quad\quad (6)\</p><p>\end{align}<br>}<br>$$<br>由（6）式可知，对于 $P_\pi(s,s’)$ 来说，一共由多少个值呢？很显然，$s$ 和 $s’$ 分别由 $\lvert S \rvert $ 个值，故 $P_\pi(s,s’)$ 会有 $\lvert S \rvert \times \lvert S \rvert  $  个。故可得矩阵如下：<br>$$<br>\large<br>P_\pi(s,s’)&#x3D; \begin{pmatrix}<br>P_\pi(s_1,s’<em>1) &amp; \cdots &amp; P_\pi(s_1,s’</em>{\lvert S \rvert})\<br>\vdots &amp; \ddots &amp; \vdots \<br>P_\pi(s_{\lvert S \rvert},s’<em>1) &amp; \cdots &amp; P_\pi(s</em>{\lvert S \rvert},s’_{\lvert S \rvert})\</p><p>\end{pmatrix} <em>{\lvert S \rvert \times \lvert S \rvert}<br>$$<br>于是，结合（3）和（6）式得：<br>$$<br>\large{<br>v_\pi(s) &#x3D; r_\pi(s) + \gamma \sum</em>{s’} P_\pi(s,s’) \cdot v_\pi(s’) \quad\quad\quad (7)<br>}<br>$$<br>令$s_i\dot{&#x3D;s},s_j \dot{&#x3D;} s’$ ，则（7） 式得：<br>$$<br>\large{<br>v_\pi(s) &#x3D; r_\pi(s) + \gamma \underbrace{\sum_{j&#x3D;1}^{\lvert S \rvert} P_\pi(s_i,s_j) }<em>{①}\cdot v_\pi(s’) \quad\quad\quad (7)<br>}<br>$$<br>由（7）式中的①式可得，正好是矩阵$P_\pi(s,s’)$ ，其中的 $P_\pi(s_i,s_j)$正是矩阵的一行。（7）式中$v_\pi(s)，r_\pi(s)，①，v_\pi(s’)$ 式（$v_\pi(s’)$本质是和$v_\pi(s)$是一样的），都分别对应一个矩阵，也即：<br>$$<br>\begin{pmatrix}<br>v_\pi(s_1)\<br>v_\pi(s_2)\<br>\vdots    \<br>v_\pi(s</em>{\lvert S \rvert})<br>\end{pmatrix} _{\lvert S \rvert \times 1}<br>&#x3D;</p><p>\begin{pmatrix}<br>r_\pi(s_1)\<br>r_\pi(s_2)\<br>\vdots    \<br>r_\pi(s_{\lvert S \rvert})<br>\end{pmatrix} _{\lvert S \rvert \times 1}</p><ul><li>\gamma</li></ul><p>\begin{pmatrix}<br>P_\pi(s_1,s’<em>1) &amp; \cdots &amp; P_\pi(s_1,s’</em>{\lvert S \rvert})\<br>\vdots &amp; \ddots &amp; \vdots \<br>P_\pi(s_{\lvert S \rvert},s’<em>1) &amp; \cdots &amp; P_\pi(s</em>{\lvert S \rvert},s’_{\lvert S \rvert})\<br>\end{pmatrix} _{\lvert S \rvert \times \lvert S \rvert}<br>\cdot </p><p>\begin{pmatrix}<br>v_\pi(s_1)\<br>v_\pi(s_2)\<br>\vdots    \<br>v_\pi(s_{\lvert S \rvert})<br>\end{pmatrix} _{\lvert S \rvert \times 1}<br>$$<br>最终化简到矩阵的运算。故，由（7）式进一步得：<br>$$<br>\large{<br>\begin{align}<br>v_\pi(s) &#x3D;&amp; r_\pi(s) + \gamma P_\pi(s,s’) \cdot v_\pi(s’) \quad\quad\quad \<br>v_\pi&#x3D;&amp; r_\pi + \gamma P_\pi \cdot v_\pi \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\qquad (8)\<br>(I-P_\pi)v_\pi &#x3D;&amp; r_\pi \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\qquad\quad\quad\quad\quad\quad\ (9) \<br>v_\pi &#x3D;&amp; (I-P_\pi)^{-1} r_\pi \qquad\qquad\qquad\qquad\qquad\qquad(10)<br>\end{align}<br>}<br>$$<br>经过以上推导，得出（10）式。其中在（8）式中$v_\pi(s’)$ 其实是 $v_\pi(s)$ 的迭代，所以直接用 $v_\pi(s)$ 替换，在（9）式中$\gamma$ 是个常数，也可忽略。从向量中可以得出复杂度$O({\lvert S \rvert}^3)$</p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20201203135011156.png" alt="image-20201203135011156"></p><h3 id="策略评估-迭代解"><a href="#策略评估-迭代解" class="headerlink" title="策略评估-迭代解"></a>策略评估-迭代解</h3><p>首先，我们来看如何使用动态规划来求解强化学习的<strong>预测问题</strong>，即求解给定策略的状态价值函数的问题。这个问题的求解过程我们通常叫做策略评估(Policy Evaluation)。</p><p>策略评估的基本思路是从<strong>任意一个状态价值函数开始</strong>，依据给定的策略，结合贝尔曼期望方程、状态转移概率和奖励同步迭代更新状态价值函数，直至其<strong>收敛</strong>，得到该策略下最终的状态价值函数。</p><p>假设我们在第$k$轮迭代已经计算出了所有的状态的状态价值，那么在第$k+1$轮我们可以利用第$k$轮计算出的<strong>状态价值</strong>计算出第$k+1$轮的状态价值。这是通过贝尔曼方程来完成的，即：<br>$$<br>\large{<br>v_{k+1}(s) &#x3D; \sum\limits_{a \in A} \pi(a|s)(R_s^a + \gamma \sum\limits_{s’ \in S}P_{ss’}^av_{k}(s’))<br> \</p><p> v_{k+1}(s) \dot{&#x3D;}\sum_{a\in A} \pi(a\mid s) \sum_{s’,r}p(s’,r \mid s,a)[r+ \gamma v_k(s’)]<br>}<br>$$<br>和上一节的式子唯一的区别是由于我们的策略$\pi$已经给定，我们不再写出，对应加上了迭代轮数的下标。我们每一轮可以对计算得到的新的状态价值函数再次进行迭代，直至状态价值的值改变**很小(收敛)**，那么我们就得出了预测问题的解，即给定策略的状态价值函数$v_\pi$。</p><h3 id="Summary-1"><a href="#Summary-1" class="headerlink" title="Summary"></a>Summary</h3><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20201203183436771.png" alt="image-20201203183436771"></p><h3 id="策略评估-（迭代解）应用"><a href="#策略评估-（迭代解）应用" class="headerlink" title="策略评估-（迭代解）应用"></a>策略评估-（迭代解）应用</h3><p>下面我们用一个具体的例子来说明策略评估的过程。</p><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20201203142402216.png" alt="image-20201203142402216"></p><p>这是一个经典的Grid World的例子。我们有一个<code>4x4</code>的16宫格。只有左上和右下的格子是终止格子。该位置的价值固定为0，个体如果到达了该2个格子，则停止移动，此后每轮奖励都是0。个体在16宫格其他格的每次移动，得到的即时奖励R都是-1。注意个体每次只能移动一个格子，且只能上下左右4种移动选择，不能斜着走, 如果在边界格往外走，则会直接移动回到之前的边界格。衰减因子我们定义为$\gamma &#x3D;1$。由于这里每次移动，下一格都是固定的，因此所有可行的的状态转化概率$P &#x3D;1$。这里给定的策略是随机策略，即每个格子里有25%的概率向周围的4个格子移动。</p><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20201203142716568.png" alt="image-20201203142716568"></p><p>首先我们初始化所有格子的状态价值为0，如上图$k&#x3D;0$的时候。现在我们开始策略迭代了。由于终止格子的价值固定为0，我们可以不将其加入迭代过程。</p><p><strong>在$k&#x3D;1$时</strong>，我们利用上面的贝尔曼方程先计算第二行第一个格子的价值：<br>$$<br>v_1^{(21)} &#x3D; \frac{1}{4}[(-1+0) +(-1+0)+(-1+0)+(-1+0)] &#x3D; -1<br>$$<br>第二行第二个格子的价值是：<br>$$<br>v_1^{(22)} &#x3D; \frac{1}{4}[(-1+0) +(-1+0)+(-1+0)+(-1+0)] &#x3D; -1<br>$$<br>其他的格子都是类似的，第一轮的状态价值迭代的结果如上图$k&#x3D;1$的时候。现在我们第一轮迭代完了。</p><p><strong>在$k&#x3D;1$时</strong>，还是看第二行第一个格子的价值：<br>$$<br>v_2^{(21)} &#x3D; \frac{1}{4}[(-1+0) +(-1-1)+(-1-1)+(-1-1)] &#x3D; -1.75<br>$$<br>第二行第二个格子的价值是：<br>$$<br>v_2^{(22)} &#x3D; \frac{1}{4}[(-1-1) +(-1-1)+(-1-1)+(-1-1)] &#x3D; -2<br>$$<br>最终得到的结果是上图$k&#x3D;2$的时候，第二轮迭代完毕。</p><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20201203142745214.png" alt="image-20201203142745214"></p><p><strong>在$k&#x3D;3$时</strong>：<br>$$<br>v_3^{(21)} &#x3D; \frac{1}{4}[(-1+0)+(-1-1.7) +(-1-2)+(-1-2)] &#x3D; -2.425 \<br>v_3^{(22)} &#x3D; \frac{1}{4}[(-1-1.7) +(-1-1.7)+(-1-2)+(-1-2)] &#x3D; -2.85<br>$$</p><blockquote><p>计算加和的过程 就是 上、下、左、右四个方向，其中无论哪次迭代，都有 $v_k^{11} &#x3D; 0$。</p></blockquote><p>最终得到的结果是上图$k&#x3D;3$的时候。就这样一直迭代下去，直到每个格子的策略价值改变很小（收敛）为止。这时我们就得到了所有格子的基于随机策略的状态价值。</p><p>可以看到，动态规划的策略评估计算过程并不复杂，但是如果我们的问题是一个非常复杂的模型的话，这个计算量还是非常大的。</p><h3 id="策略改进-策略改进定理"><a href="#策略改进-策略改进定理" class="headerlink" title="策略改进-策略改进定理"></a>策略改进-策略改进定理</h3><p>现在出现了这样一个问题，如果给定一个 $\pi$ 和 $\pi’$ ，我们如何判断哪一策略更好呢？采用我们以前提到的方法，就是分别计算各自对应的价值函数，然后通过判断两个价值函数的大小来判断策略的好坏。虽然能够得出结论，但是这个计算的过程也是会占用 “资源”的，能否有另外一种方式可以实现相应的功能呢？有，那就是<strong>策略改进定理。</strong></p><p><strong>策略改进定理：</strong> <strong>给定$\pi，\pi’$，如果$\forall(s) \in S,q_\pi(s,\pi’(s)) \geq v_\pi(s)$，那么，则有 $\forall s\in S,v_{\pi’}(s) \geq v_\pi(s)$。</strong></p><p>通过上面的定理可得，$q_\pi(s,a)$ 只要算出来了，那么 $v_\pi(s)$ 自然也会得出（$v_\pi(s)$ 是$q_\pi(s,a)$的加权平均），此时我们不再需要再求得 $v_{\pi’}(s)$，而是直接把$\pi’(s)$ 带入到 $q_\pi(s,a)$ 中的a中即可。</p><p>下面进行定理的一个证明：</p><p>在 $q_\pi(s,\pi’(s)) \geq v_\pi(s)$ 中，里面的 $q_\pi(s,a)$ 和 $v_\pi(s’)$ 有如下关系，我们在第一张MDP已经证明过：<br>$$<br>\large{<br>\begin{align}<br>q_\pi(s,a) &#x3D;&amp; \sum_{s’,r}P(s’,r \mid s,a)[r+ \gamma v_\pi(s’)] \<br>&#x3D;&amp;E_\pi[R_{t+1} + \gamma v_\pi(s_{t+1}) \mid S_t &#x3D; s, A_t&#x3D;a]<br>\end{align}<br>}<br>$$<br>其实，这里需要说明一点，上式中 $E_\pi$ 中的$\pi$ 严格意义上是不能带着的，对于$R_{t+1}$ ， 不是$\pi$ 控制的，详情看下图的蓝色虚线（在第一章中，我们称其未系统之间的状态转移），</p><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/hss02.svg" alt="image-20201128135658979"></p><p>当然了，对于 $R_{t+2}, R_{t+3}$ 等是需要 $\pi$ 控制的，在公式中的体现就是$v_\pi$。故，这里准确写法应该如下：<br>$$<br>\large{<br>\begin{align}<br>q_\pi(s,a) &#x3D;&amp; \sum_{s’,r}P(s’,r \mid s,a)[r+ \gamma v_\pi(s’)] \<br>&#x3D;&amp; E[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t &#x3D; s, A_t&#x3D;a]<br>\end{align}<br>}<br>$$<br><strong>证明：</strong>$\forall(s) \in S,q_\pi(s,\pi’(s)) \geq v_\pi(s)$，把 $a&#x3D;\pi’(s)$ 带入得：<br>$$<br>\large{<br>\begin{align}<br>v_\pi(s) \leq &amp; q_\pi(s,\pi’(s)) \<br>&#x3D;&amp; E[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t &#x3D; s, A_t&#x3D;\pi’(s)] \quad\quad\quad(1) \<br>&#x3D;&amp; E_{\pi’}[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t &#x3D; s] \quad\quad\quad\quad\quad\quad\quad\quad (2)</p><p>\end{align}<br>}<br>$$<br>上面式子中，在（2）式中，我们把 $\pi’$那个策略定义到 $R_{t+1}$ 上，其余的还是采取 $\pi$ 的策略。在（2）式中出现了 $v_\pi(s_{t+1})$ ，再带入$v_\pi(s) \leq q_\pi(s,\pi’(s))$  得：<br>$$<br>\large{<br>\begin{align}<br>v_\pi(s) \leq &amp; q_\pi(s,\pi’(s)) \<br>&#x3D;&amp; E[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t &#x3D; s, A_t&#x3D;\pi’(s)] \quad\quad\quad(1) \<br>&#x3D;&amp; E_{\pi’}[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t &#x3D; s] \quad\quad\quad\quad\quad\quad\quad\quad\ (2) \</p><p>\leq &amp; E_{\pi’}[R_{t+1} + \gamma q_\pi(S_{t+1},\pi’(S_{t+1})) \mid S_t &#x3D; s] \quad\quad\quad\quad\quad\ (3)</p><p>\end{align}<br>}<br>$$<br>再把$q_\pi(s_{t+1},\pi’(s_{t+1}))$ 带入（2）式得：<br>$$<br>\large{<br>\begin{align}<br>v_\pi(s) \leq &amp; q_\pi(s,\pi’(s)) \<br>&#x3D;&amp; E[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t &#x3D; s, A_t&#x3D;\pi’(s)] \quad\quad\quad(1) \<br>&#x3D;&amp; E_{\pi’}[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t &#x3D; s] \quad\quad\quad\quad\quad\quad\quad\quad\ (2) \</p><p>\leq &amp; E_{\pi’}[R_{t+1} + \gamma q_\pi(S_{t+1},\pi’(S_{t+1})) \mid S_t &#x3D; s] \quad\quad\quad\quad\quad\ (3) \</p><p>&#x3D;&amp;E_{\pi’}[R_{t+1} + \gamma E_{\pi’}[R_{t+2} + \gamma v_\pi(S_{t+2}) \mid S_{t+1}] \mid S_t&#x3D;s ]\quad\quad\ (4)</p><p>\end{align}<br>}<br>$$<br>有一点注意，在（4）式中 $S_{t+1}$ 没有写成等于多少的形式，这里只是对于$S_{t+2}$ 的前提条件，表明其不能独立出现。然后再对（4）展开得：<br>$$<br>\large{<br>\begin{align}<br>v_\pi(s) \leq &amp; q_\pi(s,\pi’(s)) \<br>&#x3D;&amp; E[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t &#x3D; s, A_t&#x3D;\pi’(s)] \quad\quad\quad(1) \<br>&#x3D;&amp; E_{\pi’}[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t &#x3D; s] \quad\quad\quad\quad\quad\quad\quad\quad\ (2) \</p><p>\leq &amp; E_{\pi’}[R_{t+1} + \gamma q_\pi(S_{t+1},\pi’(S_{t+1})) \mid S_t &#x3D; s] \quad\quad\quad\quad\quad\ (3) \</p><p>&#x3D;&amp;E_{\pi’}[R_{t+1} + \gamma E_{\pi’}[R_{t+2} + \gamma v_\pi(S_{t+2}) \mid S_{t+1}] \mid S_t&#x3D;s ]\quad\quad\ (4) \</p><p>&#x3D;&amp;E_{\pi’}[R_{t+1} + \gamma E_{\pi’}[R_{t+2} \mid S_{t+1}] + \gamma^2 E_{\pi’}[v_\pi(S_{t+2}) \mid S_{t+1}] \mid S_t&#x3D;s ]\quad\quad\ (5)</p><p>\end{align}<br>}<br>$$<br>对于（5）式，出现了“期望套期望”，期望的期望还是期望，并且都是 $E{\pi’}$，故（5）式可以进一步化简得：<br>$$<br>\large{<br>\begin{align}<br>v_\pi(s) \leq &amp; q_\pi(s,\pi’(s)) \<br>&#x3D;&amp; E[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t &#x3D; s, A_t&#x3D;\pi’(s)] \quad\quad\quad(1) \<br>&#x3D;&amp; E_{\pi’}[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t &#x3D; s] \quad\quad\quad\quad\quad\quad\quad\quad\ (2) \</p><p>\leq &amp; E_{\pi’}[R_{t+1} + \gamma q_\pi(S_{t+1},\pi’(S_{t+1})) \mid S_t &#x3D; s] \quad\quad\quad\quad\quad\ (3) \</p><p>&#x3D;&amp;E_{\pi’}[R_{t+1} + \gamma E_{\pi’}[R_{t+2} + \gamma v_\pi(S_{t+2}) \mid S_{t+1}] \mid S_t&#x3D;s ]\quad\quad\ (4) \</p><p>&#x3D;&amp;E_{\pi’}[R_{t+1} + \gamma E_{\pi’}[R_{t+2} \mid S_{t+1}] + \gamma^2 E_{\pi’}[v_\pi(S_{t+2}) \mid S_{t+1}] \mid S_t&#x3D;s ]\quad\quad\ (5) \</p><p>&#x3D;&amp;E_{\pi’}[R_{t+1} + \gamma R_{t+2}  + \gamma^2v_\pi(S_{t+2}) \mid S_t&#x3D;s ]\quad\quad\ (6)</p><p>\end{align}<br>}<br>$$<br>到这里，只走了两步，可以看到，由 $\pi’$ 一开始的只控制 $R_{t+1}$ ，走完两步后，$R_{t+2}$ 也属于 $\pi’$ 控制，接下来继续走的话，$\pi’$ 控制的 $R$ 会越来越多，$\pi$ 控制的 $R$ 越来越少。<br>$$<br>\large{<br>\begin{align}<br>v_\pi(s) \leq &amp; q_\pi(s,\pi’(s)) \<br>&#x3D;&amp; E[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t &#x3D; s, A_t&#x3D;\pi’(s)] \quad\quad\quad(1) \<br>&#x3D;&amp; E_{\pi’}[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t &#x3D; s] \quad\quad\quad\quad\quad\quad\quad\quad\ (2) \</p><p>\leq &amp; E_{\pi’}[R_{t+1} + \gamma q_\pi(S_{t+1},\pi’(S_{t+1})) \mid S_t &#x3D; s] \quad\quad\quad\quad\quad\ (3) \</p><p>&#x3D;&amp; E_{\pi’}[R_{t+1} + \gamma E_{\pi’}[R_{t+2} + \gamma v_\pi(S_{t+2}) \mid S_{t+1}] \mid S_t&#x3D;s ]\quad\quad\ (4) \</p><p>&#x3D;&amp; E_{\pi’}[R_{t+1} + \gamma E_{\pi’}[R_{t+2} \mid S_{t+1}] + \gamma^2 E_{\pi’}[v_\pi(S_{t+2}) \mid S_{t+1}] \mid S_t&#x3D;s ]\quad\quad\ (5) \</p><p>&#x3D;&amp; E_{\pi’}[R_{t+1} + \gamma R_{t+2}  + \gamma^2v_\pi(S_{t+2}) \mid S_t&#x3D;s ]\quad\quad\ (6)\</p><p>\leq &amp; E_{\pi’}[R_{t+1} + \gamma R_{t+2}  +\gamma^2 R_{t+3} + \gamma^3v_\pi(S_{t+3}) \mid S_t&#x3D;s ]\quad\quad\quad (7)\</p><p>\vdots \</p><p>\leq &amp; E_{\pi’} \underbrace{[R_{t+1} + \gamma R_{t+2}  +\gamma^2 R_{t+3} + \gamma^4 R_{t+4} + \cdots}_{G_t} \mid S_t&#x3D;s ] \quad\quad\ (8)\ </p><p>&#x3D;&amp; v_{\pi’}(s)</p><p>\end{align}<br>}<br>$$<br>故得到：$v_\pi’(s) \geq v_\pi(s)$， 得证。</p><h3 id="Summary-2"><a href="#Summary-2" class="headerlink" title="Summary"></a>Summary</h3><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20201203210853298.png" alt="image-20201203210853298"></p><h3 id="策略改进-贪心策略"><a href="#策略改进-贪心策略" class="headerlink" title="策略改进-贪心策略"></a>策略改进-贪心策略</h3><p>这节就是利用策略改进定理提出一种策略改进的方法——<strong>贪心策略</strong>（Greedy Policy）。对于 $\forall s \in S$，定义如下公式：<br>$$<br>\large{<br>\pi’(s) &#x3D; \underset{a}{argmax}\ q_\pi(s,a) \quad\quad (1)<br>}<br>$$<br>上面式子的意识是说，根据我们上节课所说的，从一个策略$\pi$，经过策略评估得到一些 $\pi’$ ，如下图所示。并从这些 $\pi’$ 中选择一个最大的$q_\pi(s,a)$。$\underset{a}{argmax}$ 表示能够使得表达式的值最大化的 $a$。</p><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/pe.svg"></p><p>由在第一章讲到的 $v_\pi(s)$ 和 $q_\pi(s,a)$ 的 关系得：$v_\pi(s) \leq \underset{a}max \ q_\pi(s,a)$ 。又因由（1）式可得， $q_\pi(s,\pi’(s)) &#x3D; \underset{a}max \ q_\pi(s,a)$ 故得：<br>$$<br>\large{<br>v_\pi(s) \leq \underset{a}max \ q_\pi(s,a) &#x3D;q_\pi(s,\pi’(s)) \<br>v_\pi(s) \leq q_\pi(s,\pi’(s)) \quad\quad\quad \quad \qquad\quad\quad \qquad\quad\quad<br>(2)<br>}<br>$$<br>（2）式正好满足上节我们说的策略改进定理。<strong>故由策略定理得知：对于 $\forall s \in S$，$v_{\pi’}(s) \geq v_\pi(s)$。</strong></p><p>如果在某一个时刻，一直迭代，如果出现了 $ v_\pi(s)&#x3D;v_{\pi’}(s)$ ，这种情况，也就是说明 $v_\pi(s)$ 此时已经不能再好了，并且此时$ v_\pi(s)&#x3D;v_{\pi’}(s) &#x3D;v_*$。如下图所示：</p><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/veq.svg"></p><blockquote><p><strong>证明</strong>：如果$v_{\pi’}&#x3D;v_\pi$ 那么，$ v_\pi(s)&#x3D;v_{\pi’}(s) &#x3D;v_*$</p></blockquote><p>证：由 $v_{\pi’}&#x3D;v_\pi$， 可以得出 $q_{\pi’}&#x3D;q_\pi。$$\forall s\in S$，由$v_\pi(s) &#x3D; \sum_{a\in A} \pi(a\mid s) ·q_\pi(s,a) $可得：<br>$$<br>\large{<br>\begin{align}<br>v_{\pi’}(s) &#x3D;&amp; \sum_{a\in A} {\pi’}(a\mid s) ·q_{\pi’}(s,a)\<br>&#x3D;&amp; \sum_{a\in A} {\pi’}(a\mid s) ·q_{\pi}(s,a) \quad\quad\quad\ (3) \  </p><p>\end{align}<br>}<br>$$<br>因为前面说的 $\pi’$ 都是选择一个最优的策略，也就是<strong>确定性策略</strong>，如下图所示：</p><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/hss02.svg" alt="image-20201128135658979"></p><p>假如我们选择<code>a3</code>这个策略为 $\pi’$ 。故在（3）式中的加和可以去掉了，因为是确定性策略，那么选择<code>a2</code> 和 <code>a1</code>的概率就是0，例缩当然加和后只剩下<code>a3</code> 这个策略$\pi’$，故继续推导得：<br>$$<br>\large{<br>\begin{align}<br>v_{\pi’}(s) &#x3D;&amp; \sum_{a\in A} {\pi’}(a\mid s) ·q_{\pi’}(s,a)\<br>&#x3D;&amp; \sum_{a\in A} {\pi’}(a\mid s) ·q_{\pi}(s,a) \quad\quad\quad\ (3) \<br>&#x3D;&amp; q_\pi(s,\pi’(s)) \quad\quad\quad\quad\quad\quad\quad\quad\ (4)  \</p><p>\end{align}<br>}<br>$$<br>由（4）式和（2）中$\underset{a}max \ q_\pi(s,a) &#x3D;q_\pi(s,\pi’(s))$ 得：<br>$$<br>\large{<br>\begin{align}<br>v_{\pi’}(s) &#x3D;&amp; \sum_{a\in A} {\pi’}(a\mid s) ·q_{\pi’}(s,a)\<br>&#x3D;&amp; \sum_{a\in A} {\pi’}(a\mid s) ·q_{\pi}(s,a) \quad\quad\quad\ (3) \<br>&#x3D;&amp; q_\pi(s,\pi’(s)) \quad\quad\quad\quad\quad\quad\quad\quad\quad (4)  \<br>&#x3D;&amp; \underset{a}max \ q_\pi(s,a) \qquad\quad\quad\quad\quad\quad\quad\ (5)\</p><p>\end{align}<br>}<br>$$<br>（5）式再由$q_\pi(s,a) &#x3D;\sum_{s’,r}P(s’,r \mid s,a)[r+ \gamma v_\pi(s’)] $ 可得：<br>$$<br>\large{<br>\begin{align}<br>v_{\pi’}(s) &#x3D;&amp; \sum_{a\in A} {\pi’}(a\mid s) ·q_{\pi’}(s,a)\<br>&#x3D;&amp; \sum_{a\in A} {\pi’}(a\mid s) ·q_{\pi}(s,a) \quad\quad\quad\ (3) \<br>&#x3D;&amp; q_\pi(s,\pi’(s)) \quad\quad\quad\quad\quad\quad\quad\quad\quad (4)  \<br>&#x3D;&amp; \underset{a}max \ q_\pi(s,a) \qquad\quad\quad\quad\quad\quad\quad\ (5)\<br>&#x3D;&amp; \underset{a}max \sum_{s’,r}P(s’,r \mid s,a)[r+ \gamma v_\pi(s’)] \quad (6) \<br>\end{align}<br>}<br>$$<br>在（6）式中，又因为题设中，$v_{\pi’}&#x3D;v_\pi$ 故带入得：<br>$$<br>\large{<br>\begin{align}<br>v_{\pi’}(s) &#x3D;&amp; \sum_{a\in A} {\pi’}(a\mid s) ·q_{\pi’}(s,a)\<br>&#x3D;&amp; \sum_{a\in A} {\pi’}(a\mid s) ·q_{\pi}(s,a) \quad\quad\quad\ (3) \<br>&#x3D;&amp; q_\pi(s,\pi’(s)) \quad\quad\quad\quad\quad\quad\quad\quad\quad (4)  \<br>&#x3D;&amp; \underset{a}max \ q_\pi(s,a) \qquad\quad\quad\quad\quad\quad\quad\ (5)\<br>&#x3D;&amp; \underset{a}max \sum_{s’,r}P(s’,r \mid s,a)[r+ \gamma v_\pi(s’)] \quad (6) \<br>&#x3D;&amp; \underset{a}max \sum_{s’,r}P(s’,r \mid s,a)[r+ \gamma v_{\pi’}(s’)] \quad (7)<br>\end{align}<br>}<br>$$<br>故此时我们得到：<br>$$<br>\large<br>v_{\pi’}(s) &#x3D; \underset{a}max \sum_{s’,r}P(s’,r \mid s,a)[r+ \gamma v_{\pi’}(s’)] \quad(8)<br>$$<br>并且我们由贝尔曼最优方程得：<br>$$<br>\large{<br>v_*(s)&#x3D;\underset{a}{max}\sum_{s’,r}P(s’,r \mid s,a)[r+\gamma v_\pi(s’)] \quad\quad(9) \</p><p>}<br>$$<br>故由（8）式和（9）式，以及题设$v_{\pi’}&#x3D;v_\pi$ 得证：<br>$$<br> \large<br> v_\pi(s)&#x3D;v_{\pi’}(s) &#x3D;v_*<br>$$</p><h3 id="Summary-3"><a href="#Summary-3" class="headerlink" title="Summary"></a>Summary</h3><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20201205145118572.png" alt="image-20201205145118572"></p><h2 id="价值迭代"><a href="#价值迭代" class="headerlink" title="价值迭代"></a>价值迭代</h2><h3 id="策略迭代的缺点"><a href="#策略迭代的缺点" class="headerlink" title="策略迭代的缺点"></a>策略迭代的缺点</h3><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20201205150228362.png" alt="image-20201205150228362"></p><p>从上图可以发现，在策略迭代中， 其实进行了两次策略循环，第一层是在策略评估中，第二层是策略迭代这一层。故，如果迭代次数过多的化，其实是很低效率的。<strong>策略评估的一个缺点是每一次迭代过程中都涉及了策略评估。</strong></p><h3 id="价值迭代-1"><a href="#价值迭代-1" class="headerlink" title="价值迭代"></a>价值迭代</h3><p>根据以上， 这里我们讨论一种极端的情况，只计算第一次的价值函数 $v_1(s)$， 从此进行截断，后续不在计算，也即是说，只对 $v_1(s)$ 进行策略评估，并把这个算法称为<strong>价值迭代</strong>。如图：</p><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/jieduan1.svg"></p><p>如下图所示，价值迭代只计算 从状态$s$ 到$s’$的一条分支（$v_1(s)$），假定以最右侧分支为例，这是策略评估的<strong>一步</strong>，加上策略改进后，其实只走了“半步”，也就是从$s$ 状态走到a3（假定选择了action a3），此时利用策略改进算法即可用a3处的$q_\pi(s,a)$ ，并得到$v_\pi(s)$ 和 $v_{\pi’}(s)$的大小关系，以下图为例，假设a3为$v_\pi(s)$，则a2或者a1就是$v_{\pi’}(s)$。</p><p>也即：如果按照策略迭代的方法，需要计算完所有v1（选择a1的$v_\pi(s)-&gt;v_{\pi’}(s)$），v2，v3，如上图所示。但是依据价值迭代 + 策略改进，只需要计算v1的“半步”即可，因为根据策略改进定理，只需要计算出$q_\pi(s,a)$，即可得出$v_\pi(s)$ 和 $v_{\pi’}(s)$的大小关系。</p><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/hss02.svg" alt="image-20201128135658979"></p><p>从 <strong>策略评估-（迭代解）应用</strong>小节 中可以知道，这个会有多个v1（注意：v1指的是多个状态，比如$v1^{21}$是第一次迭代中第二行第一个格子的状态，这里的“1”其实指的是迭代次数，并不一个状态）, 在这里选一个最大$v_\pi$作为$v*$ 。即完成了一次迭代。下面的公式，其是正是贝尔曼最优方程（求$v*$ ，$q*$）的 求$v*$的过程，同时也是价值迭代的算法公式：<br>$$<br>\large{<br> v_{k+1}(s) \dot{&#x3D;}\underset{a}{max} \sum_{s’,r}p(s’,r \mid s,a)[r+ \gamma v_k(s’) \quad\quad(1)<br>}<br>$$<br>从上面（1）式中，从贝尔曼最优方程的较多来说，这里变成<strong>了一条更新规则</strong>，还是以<strong>策略评估-（迭代解）应用</strong>小节的例子举例，只计算v1，把“每个格子”的数据更新了一次（第一次迭代得到v1），而不需要在根据v1的数据j计算v2，再进行更新了。</p><p>除此之外，可以把（1）式和 “<strong>策略评估-迭代解</strong>”的更新策略$ v_{k+1}(s) \dot{&#x3D;}\sum_{a\in A} \pi(a\mid s) \sum_{s’,r}p(s’,r \mid s,a)[r+ \gamma v_k(s’)]$比较，唯一不同的是，在策略评估-迭代解中，对于 $s$状态的下一步<code>action</code> 需要根据概率求得，而在价值迭代中，直接在v1中选择最大的<code>action</code>，故概率此时为1。即：<strong>价值迭代是极端情况下的策略迭代</strong>。</p><p>根据上面描述的极端情况，其实就是价值迭代，总结如下：</p><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20201205213726436.png" alt="image-20201205213726436"></p><h3 id="异步动态规划"><a href="#异步动态规划" class="headerlink" title="异步动态规划"></a>异步动态规划</h3><p>异步动态规划，也叫就地迭代，它是基于价值迭代的一种算法。在样本空间比较大的情况下，即使只进行一次迭代，也会花费很长的时间，还是以<strong>策略评估-（迭代解）应用</strong> 小节的例子，比如下图：</p><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20201205220225280.png" alt="image-20201205220225280"></p><p>如果，格子的数量及其多，即使只计算v1，（更新一次），迭代一次，花费时间也是巨大的。所以就出现了一种算法，这种算法，只随机找到“其中的一个格子” 进行更新，这就是<strong>异步动态规划</strong>。然而，为了收敛，异步算法必须不断的更新所有状态的值。</p><h2 id="广义策略迭代"><a href="#广义策略迭代" class="headerlink" title="广义策略迭代"></a>广义策略迭代</h2><p>广义策略迭代，其实就包含了前面所说的一般的策略迭代，价值迭代，还有就地迭代。如下面的图，如果都走到“顶”，那就是一般的策略迭代。如果走不到“顶”就可能是其他的迭代。</p><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20201205221614255.png" alt="image-20201205221614255"></p><p>按照下面的例子和类比，就很容易理解了，可以把这个广义策略迭代类比买房：</p><p><strong>策略迭代</strong>：全款买房，以旧换新。</p><p><strong>价值迭代</strong>：首付，以旧换新。</p><p><strong>就地策略迭代</strong>：几乎0首付，以旧换新。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://www.cnblogs.com/pinard/p/9463815.html">https://www.cnblogs.com/pinard/p/9463815.html</a></p><p><a href="https://www.bilibili.com/video/BV1nV411k7ve?t=1738">https://www.bilibili.com/video/BV1nV411k7ve?t=1738</a></p><p><a href="https://www.davidsilver.uk/wp-content/uploads/2020/03/DP.pdf">https://www.davidsilver.uk/wp-content/uploads/2020/03/DP.pdf</a></p><p><a href="http://www.incompleteideas.net/book/the-book.html">http://www.incompleteideas.net/book/the-book.html</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;强化学习-动态规划-DP&quot;&gt;&lt;a href=&quot;#强化学习-动态规划-DP&quot; class=&quot;headerlink&quot; title=&quot;强化学习-动态规划-DP&quot;&gt;&lt;/a&gt;强化学习-动态规划-DP&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;作者：YJLAugus  博客： </summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://zhaohongqiangsoliva.github.io/public/2022/08/16/2021-10-02-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-01-MDPs/"/>
    <id>http://zhaohongqiangsoliva.github.io/public/2022/08/16/2021-10-02-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-01-MDPs/</id>
    <published>2022-08-16T09:18:55.584Z</published>
    <updated>2022-08-16T00:56:05.184Z</updated>
    
    <content type="html"><![CDATA[<h1 id="强化学习入门基础-马尔达夫决策过程（MDP）"><a href="#强化学习入门基础-马尔达夫决策过程（MDP）" class="headerlink" title="强化学习入门基础-马尔达夫决策过程（MDP）"></a>强化学习入门基础-马尔达夫决策过程（MDP）</h1><blockquote><p>作者：YJLAugus  博客： <a href="https://www.cnblogs.com/yjlaugus">https://www.cnblogs.com/yjlaugus</a> 项目地址：<a href="https://github.com/YJLAugus/Reinforcement-Learning-Notes%EF%BC%8C%E5%A6%82%E6%9E%9C%E6%84%9F%E8%A7%89%E5%AF%B9%E6%82%A8%E6%9C%89%E6%89%80%E5%B8%AE%E5%8A%A9%EF%BC%8C%E7%83%A6%E8%AF%B7%E7%82%B9%E4%B8%AA%E2%AD%90Star%E3%80%82">https://github.com/YJLAugus/Reinforcement-Learning-Notes，如果感觉对您有所帮助，烦请点个⭐Star。</a></p></blockquote><h2 id="MDP背景介绍"><a href="#MDP背景介绍" class="headerlink" title="MDP背景介绍"></a>MDP背景介绍</h2><h3 id="Random-Variable"><a href="#Random-Variable" class="headerlink" title="Random Variable"></a>Random Variable</h3><p><strong>随机变量（Random Variable）</strong>，通常用大写字母来表示一个随机事件。比如看下面的例子：</p><p>$X$: 河水是咸的</p><p>$Y$: 井水是甜的</p><p>很显然，$X$, $Y$两个随机事件是没有关系的。也就是说$X$和$Y$之间<strong>是相互独立</strong>的。记作：<br>$$<br>\large<br>X \bot Y<br>$$</p><h3 id="Stochastic-Process"><a href="#Stochastic-Process" class="headerlink" title="Stochastic Process"></a>Stochastic Process</h3><p>对于一类随机变量来说，它们之间存在着某种关系。比如：</p><p>$S_{t}$：表示在 $t$ 时刻某支股票的价格，那么 $S_{t+1}$ 和 $S_t$ 之间一定是有关系的，至于具体什么样的关系，这里原先不做深究，但有一点可以确定，两者之间一定存在的一种关系。随着时间 $t$ 的变化，可以写出下面的形式：<br>$$<br>\large<br>…S_t, S_{t+1},S_{t+2}…<br>$$<br>这样就生成了一组随机变量，它们之间存在着一种相当复杂的关系，也就是说，各个随机变量之间存在着关系，即不相互独立。由此，我们会把按照某个时间或者次序上的一组不相互独立的随机变量的这样一个整体作为研究对象。这样的话，也就引出了另外的一个概念：<strong>随机过程（Stochastic Process）</strong>。也就是说随机过程的研究对象不在是单个的随机变量，而是一组随机变量，并且这一组随机变量之间存在着一种非常紧密的关系（不相互独立）。记作：<br>$$<br>\large<br>\lbrace S_t \rbrace ^\infty_{t&#x3D;1}<br>$$</p><h3 id="Markov-Chain-x2F-Process"><a href="#Markov-Chain-x2F-Process" class="headerlink" title="Markov Chain&#x2F;Process"></a>Markov Chain&#x2F;Process</h3><p><strong>马尔科夫链（Markov Chain）</strong>即马尔可夫过程，是一种特殊的随机过程——具备马尔可夫性的随机过程。</p><ul><li>马尔可夫性：（Markov Property）: 还是上面股票的例子，如果满足 $P(S_{t+1} \mid S_t,S_{t-1}…S_1) &#x3D; P(S_{t+1}\mid S_t)$，即具备了马尔可夫性。简单来说，$S_{t+1}$ 和$S_t$之间存在关系，和以前的时刻的没有关系，即只和“最近的状态” 有关系。</li><li>现实例子：下一个时刻仅依赖于当前时刻，跟过去无关。比如：一个老师讲课，明天的讲课状态一定和今天的状态最有关系，和过去十年的状态基本就没关系了。</li><li>最主要考量：为了简化计算。$P(S_{t+1} \mid S_t,S_{t-1}…S_1) &#x3D; P(S_{t+1}\mid S_t)$ 如果 $S_{t+1}$ 和 $S_t,S_{t-1}…S_1$ 都有关系的话，计算的话就会爆炸了。</li></ul><p>马尔可夫链&#x2F;过程 即满足马尔可夫性质的随机过程，记作：<br>$$<br>\large P(S_{t+1}) \mid S_t,S_{t-1}…S_1) &#x3D; P(S_{t+1}\mid S_t)<br>$$</p><h3 id="State-Space-Model"><a href="#State-Space-Model" class="headerlink" title="State Space Model"></a>State Space Model</h3><p><strong>状态空间模型（State Space Model）</strong>，常应用于 HMM,Kalman Filterm Particle Filter，关于这几种这里不做讨论。在这里就是指马尔可夫链 + 观测变量，即<code>Markov Chain + Obervation</code></p><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/spm.svg" alt="spm"></p><p>如上图所示，s1-s2-s3为马尔可夫链，a1, a2, a3为观测变量，以a2为例，a2只和s2有关和s1, s3无关。状态空间模型可以说是由马尔可夫链演化而来的模型。记作：</p><center>Markov Chain + Obervation</center><h3 id="Markov-Reward-Process"><a href="#Markov-Reward-Process" class="headerlink" title="Markov Reward Process"></a>Markov Reward Process</h3><p><strong>马尔可夫奖励过程（Markov Reward Process）</strong>，即马尔可夫链+奖励，即：<code>Markov Chain + Reward</code>。如下图：</p><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/reward.svg" alt="图片描述"></p><p>举个例子，比如说你买了一支股票，然后你每天就会有“收益”，当然了这里的收益是泛化的概念，收益有可能是正的，也有可能是负的，有可能多，有可能少，总之从今天的状态$S_t$ 到明天的状态 $S_{s+1}$  ，会有一个<code>reward</code>。记作：</p><center>Markov Chain + Reward</center><h3 id="Markov-Decision-Process"><a href="#Markov-Decision-Process" class="headerlink" title="Markov Decision Process"></a>Markov Decision Process</h3><p><strong>马尔可夫决策过程（Markov Decision Process）</strong>，即马尔可夫奖励过程的基础上加上<code>action</code>，即：<code>Markov Chain + Reward + action</code>。如果还用刚才的股票为例子的话，我们只能每天看到股票价格的上涨或者下降，然后看到自己的收益，但是无法操作股票的价格的，只有看到份，只是一个“小散户”。这里的马尔可夫决策过程相当于政策的制定者，相当于一个操盘手，可以根据不同的状态而指定一些政策，也就相当于 action。</p><p>在马尔可夫决策过程中，所有的<strong>状态</strong>是我们看成离散的，有限的集合。所有的<strong>行为</strong>也是离散有限的集合。记作：</p><p>$$<br>\large<br>\enclose{box}{<br>\it S: \quad state \quad set \quad \quad \quad\quad\quad\quad\quad\quad\quad\quad S_t \<br>\it A: \quad action \quad set ,\quad \quad \forall s \in S,A_{(s)} \quad\quad A_t\<br>\it R: \quad reward \quad set \quad\quad\quad\quad\quad\quad\quad\quad\quad R_t, R_{(t+1)} \<br>}<br>$$<br>对于上述公式简单说明，$S_t$ 用来表示某一个时刻的状态。$A_{(s)}$ 表示在<strong>某一个状态</strong>时候的行为 ，这个行为一定是基于某个状态而言的，假设在$t$ 时刻的状态为$S$ 此时的<code>action</code>记作 $A_t$ 。$R_t 和 R_{(t+1)}$ 只是记法不同，比如下面的例子：从$S_t$状态经过 $A_t$ 到$S_{t+1}$状态，获得的奖励一般记作$R_{(t+1)}$。 也就是说$S_t$， $A_t$ ，$R_{(t+1)}$ 是配对使用的。</p><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/coupleR.svg"></p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/sum01.png"></p><h2 id="MDP动态特性"><a href="#MDP动态特性" class="headerlink" title="MDP动态特性"></a>MDP动态特性</h2><h3 id="Markov-Chain"><a href="#Markov-Chain" class="headerlink" title="Markov Chain"></a>Markov Chain</h3><p>马尔可夫链只有一个量——<strong>状态</strong>。比如 $S\in(s_1,s_2,s_3,s_4,s_5,s_6,s_7,s_8,s_9,s_{10})$ ，在状态集合中一共有十个状态，每个状态之间可以互相转化，即可以从一个状态转移到另外一个状态，当然，“另外的状态” 也有可能是当前状态本身。如下图所示，s1状态到可以转移到s2状态，s1状态也可以转移到自己当前的状态，当然s1也有可能转移到s3，s4，s5，状态，下图中没有给出。</p><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/recg.svg"></p><p>根据上面的例子，我们可以把所有的状态写成矩阵的形式，就成了<strong>状态转移矩阵</strong>。用状态转移矩阵来描述马尔可夫链的<strong>动态特性</strong>。以上面的状态集合$S\in(s_1,s_2,s_3,s_4,s_5,s_6,s_7,s_8,s_9,s_{10})$ 为例，可得一个 $10\times10$ 的矩阵。如下图所示：<br>$$<br>\large<br>\begin{bmatrix}<br>    s_1s_1 &amp;… &amp; s_1s_{10} \<br>    \vdots &amp; \vdots &amp; \vdots \</p><pre><code>s_&#123;10&#125;s_1  &amp; ... &amp; s_&#123;10&#125;s_&#123;10&#125;\\</code></pre><p>\end{bmatrix}<br>$$<br>由上面的例子可知，在状态转移的过程中，对于下一个状态转移是有概率的，比如说s1转移到到s1状态的概率可能是0.5，s1有0.3的概率转移到s2状态。⻢尔科夫过程是⼀个⼆元组（S， P） ， 且满⾜： <strong>S是有限状态集合， P是状态转移概率。</strong>   可得：<br>$$<br>\large<br>P&#x3D;<br>\begin{bmatrix}<br>    P_{11} &amp;… &amp; P_{1n} \<br>    \vdots &amp; \vdots &amp; \vdots \</p><pre><code>P_&#123;n1&#125;  &amp; ... &amp; P_&#123;nn&#125;\\</code></pre><p>\end{bmatrix}<br>$$<br>一个简单的例子：如图2.2所⽰为⼀个学⽣的7种状态{娱乐， 课程1， 课程2， 课程3， 考过， 睡觉， 论⽂}， 每种状态之间的转换概率如图所⽰。 则该⽣从课程 1 开始⼀天可能的状态序列为：  </p><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20201127124916367.png" alt="image-20201127124916367"></p><h3 id="MRP"><a href="#MRP" class="headerlink" title="MRP"></a>MRP</h3><p>在 MPR 中，打个比喻，更像是随波逐流的小船，没有人为的干预，小船可以在大海中随波逐流。</p><h3 id="MDP"><a href="#MDP" class="headerlink" title="MDP"></a>MDP</h3><p>在MDP中，打个比喻，更像是有人划的小船，这里相比较MRP中的小船来说，多了“人划船桨”的概念，可以认为控制船的走向。这里我们看下面的图：</p><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/dymic1.svg"></p><p>s1状态到s2状态的过程，agent从s1发出action A1，使得s1状态转移到s2状态，并从s2状态得到一个R2的奖励。其实就是下图所示的一个过程。这是一个<strong>动态的过程</strong>，由此，引出<strong>动态函数</strong>。</p><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20201127132127738.png" alt="image-20201127132127738"></p><ul><li><p><strong>动态函数：</strong> The function $p$ defines the dynamics of the MDP.  这是书上的原话，也就说，这个动态函数定义了MDP的<code>动态特性</code>，动态函数如下：<br>$$<br>\large<br>p(s’,r\mid s,a) \dot{&#x3D;} Pr\lbrace S_{t+1}&#x3D;s’,R_{t+1} &#x3D; r \mid S_{t} &#x3D;s,A_{t}&#x3D;a \rbrace<br>$$</p></li><li><p><strong>状态转移函数：</strong> 我们去掉 $r$ ，也就是<code>reward</code>，动态函数也就成了状态转移函数。</p></li></ul><p>$$<br>\large{<br>p(s’\mid s,a) \dot{&#x3D;} Pr\lbrace S_{t+1}&#x3D;s’,\mid S_{t} &#x3D;s,A_{t}&#x3D;a \rbrace \<br>  p(s’\mid s,a) &#x3D; \sum_{r\in R} p(s’\mid s,a)<br>  }<br>$$</p><ul><li><strong><code>reward</code>的动态性：</strong> 在 s 和 a 选定后，r 也有可能是不同的，即 r 也是随机变量。但是，大多数情况在 s 和 a 选定后 r 是相同的，这里只做简单的介绍。</li></ul><h3 id="Summary-1"><a href="#Summary-1" class="headerlink" title="Summary"></a>Summary</h3><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20201127135356269.png" alt="image-20201127135356269"></p><h2 id="MDP价值函数"><a href="#MDP价值函数" class="headerlink" title="MDP价值函数"></a>MDP价值函数</h2><h3 id="策略的定义"><a href="#策略的定义" class="headerlink" title="策略的定义"></a>策略的定义</h3><p>在MDP中，即马尔可夫决策过程，最重要的当然是<strong>策略（Policy）</strong>，<strong>用 $\pi$ 来表示</strong>。在策略中其主要作用的就是<code>action</code>，也即 $A_t$，需要指出的一定是，action 一定是基于某一状态 S 时。看下面的例子：</p><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/policy.svg"></p><p>即，当 $S_t &#x3D; S$ 状态时，无论 $t$ 取何值，只要遇到 $S$ 状态就 选定 $a_1$ 这个 action ，这就是一种策略，并且是确定性策略。</p><h3 id="策略的分类"><a href="#策略的分类" class="headerlink" title="策略的分类"></a>策略的分类</h3><ul><li><p><strong>确定性策略：</strong>也就是说和时间 $t$ 已经没有关系了，只和这个状态有关，只要遇到这个状态，就做出这样的选择。</p></li><li><p><strong>随机性策略：</strong>与确定性策略相对，当遇到 $S$ 状态时，可能选择 $a_1$ ，可能选择 $a_2$，也可能选择 $a_3$。只是选择action的概率不同。如下图，就是<strong>两种不同</strong>的策略:</p><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/p1.svg"></p><p>从上面两天图中，因为一个策略是基于一个状态而言的，在 $S$ 状态，可能选择 $a_1$ ，可能选择 $a_2$，也可能选择 $a_3$，故三个 <code>action</code> 之间是<strong>或</strong>的关系，所以说以上是两个策略，而不要误以为是6个策略。</p></li></ul><p>故策略可分为确定性策略和随机性策略两种。</p><p>$$<br>\large<br>Policy&#x3D; \begin{cases} 确定性策略, &amp; \text {a $\dot{&#x3D;}\pi(s)$} \ 随机性策略, &amp; \text { $\pi(a\mid s) \dot{&#x3D;} P \lbrace A_t&#x3D;a \mid S_t &#x3D; s \rbrace$} \end{cases}<br>$$</p><p>对于随机性策略而言，给定一个 $s$ ，选择<strong>一个 $a$</strong> ，也就是条件概率了。</p><blockquote><p>确定性策略可以看作是一种特殊的随机性策略，以上表-Policy1为例，选择a1的概率为1，选择a2，a3的概率都为0。</p></blockquote><h3 id="最优策略"><a href="#最优策略" class="headerlink" title="最优策略"></a>最优策略</h3><p>在所有的策略中一定存在至少一个<strong>最优策略</strong>，而且在强化学习中，<code>reward</code>的获得有<strong>延迟性（delay）</strong>，举雅达利游戏中，很多游戏只有到结束的时候才会知道是否赢了或者输了，才会得到反馈，也就是<code>reward</code>，所以这就是奖励获得延迟。当选定一个状态 $S_t$ 时，选定action $A_t$ ，因为奖励延迟的原因可能对后续的 $S_{t+1}$  等状态都会产生影响。这样，就不能用当前的reward来衡量策略的好坏、优劣。这里引入了回报和价值函数的概念。</p><h4 id="回报（-G-t-）"><a href="#回报（-G-t-）" class="headerlink" title="回报（$G_t$）"></a>回报（$G_t$）</h4><p>而是后续多个reward的加和，也就是<strong>回报</strong>，用 $G_t$ 表示 $t$ 时刻的回报。</p><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/optimp.svg"></p><p>如上图所示，此时的“回报”可以表示为：$G_t &#x3D; R_{t+1} + R_{t+2}+ \ …\ +R_T$</p><p>值得注意的是，$T$ 可能是有限的，也有可能是无限的。</p><p>举例：张三对李四做了行为，对李四造成了伤害，李四在当天就能感受到伤害，而且，这个伤害明天，后头都还是有的，但是，时间是最好的良药，随着时间的推移，李四对于张三对自己造成的伤害感觉没有那么大了，会有一个wei折扣，用 $\gamma$ 来表示。故<strong>真正的回报</strong>表示为：<br>$$<br>\large<br>G_t &#x3D; R_{t+1} + \gamma R_{t+2}+\gamma^2 R_{t+3} \ …\ +\gamma^{T-t-1}R_T &#x3D; \sum_{i&#x3D;0}^{\infty}\gamma^i R_{t+i+1} \quad \quad \gamma\in[0,1],\quad (T\rightarrow\infty)<br>$$<br><strong>用 $G_t$ 来衡量一个策略的好坏，$G_t$ 大的策略就好，反之。</strong></p><p>但是使用 $G_t$ 也不能很好的衡量策略的好坏，比如一个最大的问题是，在给定一个状态后，选择一个确定的action后（这里还是在随机策略的角度），进入的下一个状态也是随机的。如下图所示：把左侧的过程放大，只给出a3下的随机状态，a1，a2也是有同样的情况，这里胜率。</p><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/Gtlim.svg"></p><p>举个例子，就像我们给一盆花浇水，水是多了还是少了，对于这盆花来说我们是不得知的，可能会少，也可能会多。这个花的状态变化也就是随机的了。从上面的例子得知，如果还是用 $G_t$ 来对一个策略来进行评估的话，至少有9中情况（随机策略，3个action选一种）。</p><p>$G_t$ 只能评估的是一个“分叉”而已（图中<code>绿色分支</code>）。而不能更好的进行评估。如下图所示：</p><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/merch.svg"></p><p>因为回报不能很好的对策略进行一个评估，由此引入了另外一个概念——价值函数。</p><h4 id="价值函数（Value-Function-）"><a href="#价值函数（Value-Function-）" class="headerlink" title="价值函数（Value Function ）"></a>价值函数（Value Function ）</h4><p>在指定一个状态 $s$ ，采取一个<code>随机策略 </code>$\pi$ ，然后加权平均，以上图为例，把9 个分叉($G_t$)加权平均。也就是<strong>期望</strong> $E$。故得出价值函数：<br>$$<br>\large<br>V_\pi(s) &#x3D; E_\pi[G_t\mid S_t &#x3D; s]<br>$$</p><h3 id="Summary-2"><a href="#Summary-2" class="headerlink" title="Summary"></a>Summary</h3><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20201128110205095.png" alt="image-20201128110205095"></p><h2 id="MDP贝尔曼期望方程"><a href="#MDP贝尔曼期望方程" class="headerlink" title="MDP贝尔曼期望方程"></a>MDP贝尔曼期望方程</h2><h3 id="价值函数分类"><a href="#价值函数分类" class="headerlink" title="价值函数分类"></a>价值函数分类</h3><p>上面提到的的价值函数其实是其中的一种，确切的可以称为 <code>状态价值函数</code>，<strong>用$v_\pi(s)$ 来表示</strong>只和状态有关系。初次之外还有另外一种价值函数，那就是<code>状态动作价值函数</code>，用$q_\pi(s,a)$这里引入的<code>action</code>。故价值函数可以分为下面的两种:<br>$$<br>Value \quad Function &#x3D; \begin{cases} v_\pi(s) &#x3D; E_\pi[G_t\mid S_t &#x3D; s], &amp; \text {only $s$ is independent variable} \ q_\pi(s,a) &#x3D; E_\pi[G_t\mid S_t &#x3D; s,A_t &#x3D; a], &amp; \text{Both $s$ and a are independent variable} \end{cases}<br>$$<br>从上面的公式中，我们可以得知，在 $v_\pi(s)$ 中，只有 $s$ 是自变量，一个 $\pi$ 其实就是一个状态$s$ 和一个action的一个映射。故，只要$\pi$ 确定了，那么$s,a$ 也就确定了，即此时的 $\pi$ 对状态 $s$ 是有限制作用的。但是，在 $q_\pi(s,a)$ 中，子变量为$s,a$ 两个，这两个自变量之间是没有特定的关系的。也就是说，$s$和$a$ 都在变，无法确定一个映射(策略) $\pi$ ,那么也就是说在 $q_\pi$ 中的$\pi$ 对于$s$ 是没有约束的。</p><h3 id="两种价值函数之间的关系"><a href="#两种价值函数之间的关系" class="headerlink" title="两种价值函数之间的关系"></a>两种价值函数之间的关系</h3><h4 id="v-pi-s-和-q-pi-s-a-之间的关系"><a href="#v-pi-s-和-q-pi-s-a-之间的关系" class="headerlink" title="$v_\pi(s)$ 和 $q_\pi(s,a)$ 之间的关系"></a>$v_\pi(s)$ 和 $q_\pi(s,a)$ 之间的关系</h4><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/Gtlim.svg"></p><p>还是以上图为例，对于 s 状态，在随机策略中有三种 action 选择，分别是 $\pi(a_1 \mid s)$，$\pi(a_1 \mid s)$，$\pi(a_1 \mid s)$，三种action(行为)对应的价值函数（此时为动作价值函数）为 $q_\pi(s,a_1)$， $q_\pi(s,a_2)$， $q_\pi(s,a_3)$。那么此时的 $v_\pi(s)$ 就等于各个action的动作状态价值函数的加和，即：<br>$$<br>v_\pi(s) &#x3D; \pi(a_1 \mid s)·q_\pi(s,a_1) +  \pi(a_2 \mid s)·q_\pi(s,a_2) +  \pi(a_3 \mid s)·q_\pi(s,a_3)<br>$$<br>这样一来我们就得出了 <strong>$v_\pi(s)$ 和 $q_\pi(s,a)$ 之间的关系</strong>，若条件已知，就可以直接计算出 $v_\pi$。<br>$$<br>\large<br>v_\pi(s) &#x3D; \sum_{a\in A} \pi(a\mid s) ·q_\pi(s,a)<br>$$<br>对于某个状态 $s$ 来说，$v_\pi \leq \underset{a}{max}\ q_\pi(s,a)$ ，$v_\pi(s)$ 是一个加权平均，实际上就是一个平均值，当然要小于等于$\ q_\pi(s,a)$的最大值。$v_\pi(s)$只有全部是最大值的时候，两者才有可能相等。比如 5，5，5，平均值是5，最大值也是5；3，4，5而言，平均值为4，但是最大值为5。注意的是，4是乘上权值后的值，换句话说也就是乘了一个概率（$\pi(a\mid s)$）。</p><h4 id="q-pi-s-a-和-v-pi-s’-之间的关系"><a href="#q-pi-s-a-和-v-pi-s’-之间的关系" class="headerlink" title="$q_\pi(s,a)$ 和 $v_\pi(s’)$  之间的关系"></a>$q_\pi(s,a)$ 和 $v_\pi(s’)$  之间的关系</h4><p>从下面图中可得，在 $q_\pi(s,a)$ 位置，（一个action）状态转移只能向“箭头”方向转移，而不能向上。如果想从下面的状态转移到上面的状态，那必须还要另外一个action。情况是一样的，就按下图来说明，经过a3后到达状态s’，此时的状态函数就是 $v_\pi(s’）$。</p><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/hss01.svg" alt="image-20201128134834753"></p><p>上面的图可知： 在确定了s 后，由随机策略action，引起“分叉”，同理，以a3为例，因为系统状态转移的随机性，也会引起分叉，也就是 s’ 的状态也是不确定的。还有一点 r 也又不确定性，如下图蓝色虚线部分。</p><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/hss02.svg" alt="image-20201128135658979"></p><p>由我们前面提到的公式也可得知：s’ 和 r 都是随机的。比如说s，a ，s’ 都是给定的，r也是不确定的。<br>$$<br>\large p(s’,r\mid s,a) \dot{&#x3D;} Pr\lbrace S_{t+1}&#x3D;s’,R_{t+1} &#x3D; r \mid S_{t} &#x3D;s,A_{t}&#x3D;a \rbrace<br>$$<br>这样一来，可得<strong>一条蓝色通路的回报</strong>：<br>$$<br>\large<br>q_\pi(s,a) &#x3D; r + \gamma v_\pi(s’)  \quad\quad\quad (1)<br>$$<br>(1)式是怎么来的呢？以上图为例，在 $q_\pi(s,a)$ 处往下走，选定一个 r ，再往下到达一个状态s’， 此时在往下还是同样的状态，就是俄罗斯套娃，以此类推。关于其中的 $\gamma v_\pi(s’) $ ，来自于 $G_t$。看下面的式子：<br>$$<br>\large{<br>G_t &#x3D; R_{t+1} + \gamma R_{t+2}+\gamma^2 R_{t+3} \ …\ +\gamma^{T-t-1}R_T \quad\quad  \gamma\in[0,1],\quad (T\rightarrow\infty) \<br>G_t &#x3D; R_{t+1} + \gamma(R_{t+2}+\gamma R_{t+3}+\gamma^2 R_{t+4}+ …)<br>}<br>$$<br>因为 $v_\pi(s)$ 来自 $G_t$ ，故类比得（1）式。</p><p>因为走每条蓝色的通路也都是由概率的，故我们需要乘上概率，同时累加求和，一个是多条蓝色通路另一个是多个s’。故得：**$q_\pi(s,a)$ 和 $v_\pi(s’)$ 之间的关系** 如下：<br>$$<br>\large<br>q_\pi(s,a) &#x3D;\sum_{s’,r}P(s’,r \mid s,a)[r+ \gamma v_\pi(s’)] \quad\quad\quad (2)<br>$$</p><h3 id="贝尔曼期望等式（方程）"><a href="#贝尔曼期望等式（方程）" class="headerlink" title="贝尔曼期望等式（方程）"></a>贝尔曼期望等式（方程）</h3><p>这样我们得到两个式子：<br>$$<br>\large{<br>v_\pi(s) &#x3D; \sum_{a\in A} \pi(a\mid s) ·q_\pi(s,a)  \quad\quad\quad\quad\quad\quad\quad\quad\ (3) \</p><p>q_\pi(s,a) &#x3D;\sum_{s’,r}P(s’,r \mid s,a)[r+ \gamma v_\pi(s’)] \quad\quad\quad (4)<br>}<br>$$<br>（4）式带入（3）得：<br>$$<br>\large<br>v_\pi(s) &#x3D; \sum_{a\in A} \pi(a\mid s) \sum_{s’,r}P(s’,r \mid s,a)[r+ \gamma v_\pi(s’)] \quad\quad\quad\quad\quad\quad (5)<br>$$<br>（3）式带入（4）得：<br>$$<br>\large<br>q_\pi(s,a) &#x3D;\sum_{s’,r}P(s’,r \mid s,a)[r+ \gamma \sum_{a’\in A} \pi(a’\mid s’)  ·q_\pi(s’,a’) ] \quad\quad (6)<br>$$<br>关于（6）式可以看下图，更容易理解：</p><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/hss03.svg"></p><p>（5）式和（6）式 被称为<strong>贝尔曼期望方程</strong>。</p><ul><li><p>一个实例：</p><p>例子是一个学生学习考试的MDP。里面实心圆位置是<strong>起点</strong>，方框那个位置是<strong>终点</strong>。上面的动作有study, Pub, Facebook, Quit, Sleep，每个状态动作对应的即时奖励R已经标出来了。我们的目标是找到最优的状态动作价值函数或者状态价值函数，进而找出最优的策略。</p><p>&lt;<a href="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img">https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img</a> src&#x3D;”<a href="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20201129160656460.png&quot;">https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20201129160656460.png&quot;</a> alt&#x3D;”image-20201129160656460” style&#x3D;”zoom:50%;” &#x2F;&gt;</p><p>为了方便，我们假设衰减因子 $\gamma &#x3D;1, \pi(a|s) &#x3D; 0.5$ 。对于终点方框位置，由于其没有下一个状态，也没有当前状态的动作，因此其状态价值函数为0，对于其他的状态（圆圈）按照从上到下，从左到右的顺序定义其状态价值函数分别是 $v_1,v_2,v_3,v_4$ ，根据（5）式 :<br>$$<br>v_\pi(s) &#x3D; \sum_{a\in A} \pi(a\mid s) \sum_{s’,r}P(s’,r \mid s,a)[r+ \gamma v_\pi(s’)] \quad\quad\quad\quad\quad\quad (5)<br>$$<br>对于$v_1$位置，我们有：$v_1 &#x3D; 0.5*(-1+v_1) +0.5*(0+v_2)$</p><p>对于$v_2$位置，我们有：$v_2 &#x3D; 0.5*(-1+v_1) +0.5*(-2+v_3)$</p><p>对于$v_3$位置，我们有：$v_3 &#x3D; 0.5*(0+0) +0.5*(-2+v_4)$</p><p>对于$v_4$位置，我们有：$v_4 &#x3D; 0.5*(10+0) +0.5*(1+0.2<em>v_2+0.4</em>v_3+0.4*v_4)$</p><p>解出这个方程组可以得到 $v_1&#x3D;-2.3, v_2&#x3D;-1.3, v_3&#x3D;2.7, v_4&#x3D;7.4$, 即每个状态的价值函数如下图：</p></li></ul><p>&lt;<a href="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img">https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img</a> src&#x3D;”<a href="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20201129162749453.png&quot;">https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20201129162749453.png&quot;</a> alt&#x3D;”image-20201129162749453” style&#x3D;”zoom:50%;” &#x2F;&gt;</p><blockquote><p>从上面可以看出，针对一个特定状体，状态价值函数计算都是基于下一个状态而言的，通俗的讲，按照“出箭头”的方向计算当前状态的价值函数。</p></blockquote><h3 id="Summary-3"><a href="#Summary-3" class="headerlink" title="Summary"></a>Summary</h3><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20201128152843746.png" alt="image-20201128152843746"></p><h2 id="MDP贝尔曼最优方程"><a href="#MDP贝尔曼最优方程" class="headerlink" title="MDP贝尔曼最优方程"></a>MDP贝尔曼最优方程</h2><h3 id="最优价值函数"><a href="#最优价值函数" class="headerlink" title="最优价值函数"></a>最优价值函数</h3><p>能够使得 $v$ 达到最大值的那个 $\pi$ ，这个  $\pi$ 被成为最优策略，进而得到<strong>最优状态价值函数</strong>。同理得到最<strong>优状态动作价值函数</strong>。<br>$$<br>\large<br>\begin{cases} v_*(s)\ \dot{&#x3D;}\ \ \underset{\pi}{max} \ v_\pi(s) &amp; \text{} \<br>q_*(s,a)\ \dot{&#x3D;}\ \ \underset{\pi}{max} \ q_\pi(s,a) &amp; \text{} &amp; \text {} \end{cases}<br>$$<br>记 $\pi_* &#x3D; \underset{\pi}{argmax} \ v_\pi(s) &#x3D; \underset{\pi}{argmax} \ q_\pi(s,a)$，含义是 $\pi_*$ 可以使得 $ v_*(s)$达到最大值，同样的，也可以使得 </p><p>$q_\pi(s,a)$ 达到最大值。</p><p>由以上公式得：<br>$$<br>\large<br>\begin{cases}v_*(s)&#x3D;\underset{\pi}{max}\ v_\pi(s)&#x3D; v_{\pi_*}(s) &amp; \text{(7)} \<br>q_*(s,a)&#x3D;\underset{\pi}{max}\ q_\pi(s,a)&#x3D; q_{\pi_*}(s,a) &amp; \text{} &amp; \text {(8)} \end{cases}<br>$$</p><blockquote><p>值得注意的一点是$ v_*(s)$ 强调的是，不管你采用的是什么策略，只要状态价值函数达到最大值，而 $v_{\pi_*}(s)$ 则更为强调的是 $\pi$ ，达到最大的状态价值函数所采取的最优的那个 $\pi$</p></blockquote><p>此时，我们再探讨一下$v_{\pi_*}(s)$ 和 $q_{\pi_*}(s,a)$ 的关系。在贝尔曼期望方程中，我们提到过 $v_\pi(s) \leq \underset{a}{max}\ q_\pi(s,a)$ ，那么在这里是不是也由类似的关系$v_{\pi_*}(s)\leq \underset{a}{max}\ q_\pi(s,a)$  成立？我们知道 $v_{\pi_*}(s)$ 是一种策略，并且是最优的策略，$q_{\pi_*}(s,a)$ 是代表一个“分支”，因为 $v_{\pi_*}(s)$ 是一个加权平均值，但同样的，和$v_\pi(s)$ 不同的是，$v_{\pi_*}(s)$ 是最优策略的加权平均，那么是不是可以把小于号去掉，写成下面的形式：<br>$$<br>\large<br>v_{\pi_*}(s)&#x3D; \underset{a}{max}\ q_\pi(s,a)<br>$$</p><p>假定 $v_{\pi_*}(s)\leq \underset{a}{max}\ q_\pi(s,a)$ 中的 $\pi_*$ 还是一个普通的策略，那么一定满足  $v_{\pi_*}(s)\leq \underset{a}{max}\ q_\pi(s,a)$ ，这一点我们已经提到过，如果说 $v_{\pi_*}(s)&lt; \underset{a}{max}\ q_\pi(s,a)$ ，说明$v_{\pi_*}(s)$ 还有提高的空间，并不是最优策略，这和条件矛盾了。所以这个小于不成立，得证：$v_{\pi_*}(s)&#x3D; \underset{a}{max}\ q_\pi(s,a)$ </p><p>详细证明过程如下：</p><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20201129190023306.png" alt="image-20201129190023306"></p><p>其实，上面的式子是由  (3)式<br>$$<br>v_\pi(s) &#x3D; \sum_{a\in A} \pi(a\mid s) ·q_\pi(s,a)  \quad\quad (3)<br>$$<br>演变而来的。$v_{\pi_*}(s)$ 直接取最大值时候和 $\underset{a}{max}\ q_\pi(s,a)$ 的最大值是相等的。也就是此时不用加权平均了，直接是 $v_\pi(a) &#x3D; q_\pi(s,a)$ 。那么从原先的(4)式能不能也得出相似<br>$$<br>q_\pi(s,a) &#x3D;\sum_{s’,r}P(s’,r \mid s,a)[r+ \gamma v_\pi(s’)] \quad\quad (4)<br>$$<br>的结论，把求和符号去掉，直接等于最大值呢？答案是否定的，因为$v_{\pi_*}(s)&#x3D; \underset{a}{max}\ q_\pi(s,a)$  是作用在<code>action</code>上的，在公式中也可以看出，换句话说，我们对于下图的a1，a2，a3这里是可以控制的。但是对于下图中的蓝色虚线部分，系统状态转移是无法控制的。</p><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/hss02.svg" alt="image-20201128135658979"></p><p>所以，原先的两组公式（3）、（4）并 结合（7）、（8）<br>$$<br>\large{<br>v_\pi(s) &#x3D; \sum_{a\in A} \pi(a\mid s) ·q_\pi(s,a)  \quad\quad\quad\quad\quad\quad\quad\quad\ (3) \</p><p>q_\pi(s,a) &#x3D;\sum_{s’,r}P(s’,r \mid s,a)[r+ \gamma v_\pi(s’)] \quad\quad\quad (4)<br>}<br>$$<br>并进行一个推导，得出另外的两组公式（9）、（10）如下：<br>$$<br>\large{<br>v_*(s)&#x3D;\underset{a}{max}\ q_*(s,a) \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad (9) \<br>q_*(s,a)&#x3D; \sum_{s’,r}P(s’,r \mid s,a)[r+\gamma v_*(s’)] \quad\quad\quad (10)<br>}<br>$$</p><h3 id="贝尔曼最优方程"><a href="#贝尔曼最优方程" class="headerlink" title="贝尔曼最优方程"></a>贝尔曼最优方程</h3><p>（10）式带入（9）式得：<br>$$<br>\large{<br>v_*(s)&#x3D;\underset{a}{max}\sum_{s’,r}P(s’,r \mid s,a)[r+\gamma v_\pi(s’)] \quad\quad(11) \</p><p>}<br>$$<br>（9）式带入（10）式得：<br>$$<br>\large<br>q_*(s,a)&#x3D; \sum_{s’,r}P(s’,r \mid s,a)[r+\gamma \underset{a’}{max}\ q_*(s’,a’) ] \quad\quad (12)<br>$$<br>（11）、（12）被称为<strong>贝尔曼最优方程</strong>。</p><ul><li><p>一个实例：还是以上面的例子讲解，我们这次以动作价值函数 $q_*(s,a)$ 为例来求解 $v_*(s),q_*(s,a) $</p><p>&lt;<a href="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img">https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img</a> src&#x3D;”<a href="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20201129160656460.png&quot;">https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20201129160656460.png&quot;</a> alt&#x3D;”image-20201129160656460” style&#x3D;”zoom:50%;” &#x2F;&gt;</p></li></ul><p>根据（12）式</p><p>$$<br>\large<br>q_*(s,a)&#x3D; \sum_{s’,r}P(s’,r \mid s,a)[r+\gamma \underset{a’}{max}\ q_*(s’,a’) ] \quad\quad (12)<br>$$<br>可得方程组如下：</p><p>$$<br>\large{\begin{align}<br>q_*(s_4, study) &amp; &#x3D; 10 \<br>q_*(s_4, pub) &amp; &#x3D; 1 + 0.2 * \underset{a’}{max}q_*(s_2, a’) + 0.4 * max_{a’}q_*(s_3, a’) + 0.4 * \underset{a’}{max}q_*(s_4, a’) \<br>q_*(s_3, sleep) &amp; &#x3D; 0  \<br>q_*(s_3, study) &amp; &#x3D; -2 + \underset{a’}{max}q_*(s_4, a’) \<br>q_*(s_2, study) &amp; &#x3D; -2 + \underset{a’}{max}q_*(s_3, a’) \<br>q_*(s_2, facebook) &amp; &#x3D; -1 + \underset{a’}{max}q_*(s_1, a’) \<br>q_*(s_1, facebook) &amp; &#x3D; -1 + \underset{a’}{max}q_*(s_1, a’) \<br>q_*(s_1, quit) &amp; &#x3D; 0 + \underset{a’}{max}q_*(s_2, a’)<br>\end{align}}<br>$$</p><p>然后求出所有的 $q_*(s,a)$，然后再利用 $v_*(s) &#x3D; \underset{a’}{max}q_*(s,a)$，就可以求出所有的  $v_*(s)$，最终结果如下图所示：</p><p>&lt;<a href="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img">https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img</a> src&#x3D;”<a href="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20201130141108720.png&quot;">https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20201130141108720.png&quot;</a> alt&#x3D;”image-20201130141108720” style&#x3D;”zoom: 67%;” &#x2F;&gt;</p><p>详细的计算过程可以看下视频 的简单分析。<a href="https://www.bilibili.com/video/BV1Fi4y157vR/">https://www.bilibili.com/video/BV1Fi4y157vR/</a></p><h3 id="Summary-4"><a href="#Summary-4" class="headerlink" title="Summary"></a>Summary</h3><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20201129210325397.png" alt="image-20201129210325397"></p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://www.bilibili.com/video/BV1RA411q7wt">https://www.bilibili.com/video/BV1RA411q7wt</a></p><p><a href="https://www.cnblogs.com/pinard/p/9426283.html">https://www.cnblogs.com/pinard/p/9426283.html</a></p><p><a href="https://www.davidsilver.uk/wp-content/uploads/2020/03/MDP.pdf">https://www.davidsilver.uk/wp-content/uploads/2020/03/MDP.pdf</a></p><p><a href="https://www.cnblogs.com/jsfantasy/p/jsfantasy.html">https://www.cnblogs.com/jsfantasy/p/jsfantasy.html</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;强化学习入门基础-马尔达夫决策过程（MDP）&quot;&gt;&lt;a href=&quot;#强化学习入门基础-马尔达夫决策过程（MDP）&quot; class=&quot;headerlink&quot; title=&quot;强化学习入门基础-马尔达夫决策过程（MDP）&quot;&gt;&lt;/a&gt;强化学习入门基础-马尔达夫决策过程（MD</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://zhaohongqiangsoliva.github.io/public/2022/08/16/hello-world/"/>
    <id>http://zhaohongqiangsoliva.github.io/public/2022/08/16/hello-world/</id>
    <published>2022-08-16T07:18:01.370Z</published>
    <updated>2022-08-16T07:18:01.370Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for</summary>
      
    
    
    
    
  </entry>
  
</feed>

<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Hexo | Hexo</title><meta name="author" content="John Doe"><meta name="copyright" content="John Doe"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="强化学习-动态规划-DP 作者：YJLAugus  博客： https:&#x2F;&#x2F;www.cnblogs.com&#x2F;yjlaugus 项目地址：https:&#x2F;&#x2F;github.com&#x2F;YJLAugus&#x2F;Reinforcement-Learning-Notes，如果感觉对您有所帮助，烦请点个⭐Star。  这里讲动态规划，主要是用动态规划来解决MDP的中最优的策略，并得出最优的价值函数。这节主要介绍两中动态规">
<meta property="og:type" content="article">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://zhaohongqiangsoliva.github.io/public/2022/08/16/2021-10-02-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-02-DP/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="强化学习-动态规划-DP 作者：YJLAugus  博客： https:&#x2F;&#x2F;www.cnblogs.com&#x2F;yjlaugus 项目地址：https:&#x2F;&#x2F;github.com&#x2F;YJLAugus&#x2F;Reinforcement-Learning-Notes，如果感觉对您有所帮助，烦请点个⭐Star。  这里讲动态规划，主要是用动态规划来解决MDP的中最优的策略，并得出最优的价值函数。这节主要介绍两中动态规">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7">
<meta property="article:published_time" content="2022-08-16T09:18:55.586Z">
<meta property="article:modified_time" content="2022-08-16T00:56:05.185Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"><link rel="shortcut icon" href="/public/img/favicon.png"><link rel="canonical" href="http://zhaohongqiangsoliva.github.io/public/2022/08/16/2021-10-02-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-02-DP/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/public/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/public/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Hexo',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-08-16 08:56:05'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.2.0"><link rel="alternate" href="/public/atom.xml" title="Hexo" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/public/archives/"><div class="headline">Articles</div><div class="length-num">5</div></a><a href="/public/tags/"><div class="headline">Tags</div><div class="length-num">0</div></a><a href="/public/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><hr/></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/public/">Hexo</a></span><div id="menus"><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">No title</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2022-08-16T09:18:55.586Z" title="Created 2022-08-16 17:18:55">2022-08-16</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2022-08-16T00:56:05.185Z" title="Updated 2022-08-16 08:56:05">2022-08-16</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="强化学习-动态规划-DP"><a href="#强化学习-动态规划-DP" class="headerlink" title="强化学习-动态规划-DP"></a>强化学习-动态规划-DP</h1><blockquote>
<p>作者：YJLAugus  博客： <a target="_blank" rel="noopener" href="https://www.cnblogs.com/yjlaugus">https://www.cnblogs.com/yjlaugus</a> 项目地址：<a target="_blank" rel="noopener" href="https://github.com/YJLAugus/Reinforcement-Learning-Notes%EF%BC%8C%E5%A6%82%E6%9E%9C%E6%84%9F%E8%A7%89%E5%AF%B9%E6%82%A8%E6%9C%89%E6%89%80%E5%B8%AE%E5%8A%A9%EF%BC%8C%E7%83%A6%E8%AF%B7%E7%82%B9%E4%B8%AA%E2%AD%90Star%E3%80%82">https://github.com/YJLAugus/Reinforcement-Learning-Notes，如果感觉对您有所帮助，烦请点个⭐Star。</a></p>
</blockquote>
<p>这里讲动态规划，主要是用动态规划来解决MDP的中最优的策略，并得出最优的价值函数。这节主要介绍两中动态规划的方法，一个是<strong>策略迭代</strong>方法，另一个是<strong>价值迭代</strong>方法。</p>
<h2 id="策略迭代"><a href="#策略迭代" class="headerlink" title="策略迭代"></a>策略迭代</h2><p>策略迭代主要分两个，一个是<strong>策略评估</strong>，一个是<strong>策略改进</strong>。定义比较简单，分别如下：</p>
<p><strong>策略评估</strong>：给定任意的一个 $\pi$ ，求出 状态价值函数 $v_\pi(s)$ 或者状态动作价值函数 $q_\pi(s,a)$。<br><strong>策略改进</strong>：依据策略评估得出来的 $v_\pi(s)$ 或者 $q_\pi(s,a)$，从其中构造出一个新的 $\pi’$ ，使得 $\pi’ \geq \pi $ ，这样就完成了一次迭代。</p>
<ul>
<li>换句话说，就是在每个状态下根据$v_\pi(s)$ 或者 $q_\pi(s,a)$选择一个最优的策略，这个策略就是 $\pi’$ 。</li>
<li>这种根据<code>原策略的价值函数</code>执行贪心算法，来构造一个更好策略的过程，我们称为策略改进。<br><strong>策略迭代</strong>： 就是递归以上两个过程，以上面的例子，得到$\pi’$的价值函数，然后再以$\pi’$的价值函数构造一个新的$\pi’’$ ，不断迭代。</li>
</ul>
<h3 id="策略评估-解析解"><a href="#策略评估-解析解" class="headerlink" title="策略评估-解析解"></a>策略评估-解析解</h3><p>策略评估，就是已知 MDP，“已知MDP”的意思是知道其动态特性，也就是 $p(s’,r \mid s,a)$，给定$\pi$ ，求$v_\pi(s)$。</p>
<p>已知$(\forall s\in S)$ ，即有多少个$s$ 就会有多少$v_\pi(s)$，可得列向量如下：<br>$$<br>\large<br>v_\pi(s)&#x3D; \begin{pmatrix}<br>v_\pi(s_1)\<br>v_\pi(s_2)\<br>\vdots    \<br>v_\pi(s_{\lvert S \rvert})</p>
<p>\end{pmatrix} _{\lvert S \rvert \times 1}<br>$$</p>
<p>上节课我们得到价值函数的公式，并进一步推导得到贝尔曼期望方程：<br>$$<br>\large<br>{\begin{align}<br>v_\pi(s) &#x3D;&amp; E_\pi[G_t \mid S_t &#x3D; s] \<br>&#x3D;&amp; E_\pi[R_{t+1} + \gamma v_\pi(S_{t+1} \mid S_t&#x3D;s)]\<br>&#x3D;&amp; v_\pi(s) &#x3D; \sum_{a\in A} \pi(a\mid s) \sum_{s’,r}p(s’,r \mid s,a)[r+ \gamma v_\pi(s’)]<br>\end{align}<br>}<br>$$<br>从上面的列向量中得知，如果我们想要求整个的$v_\pi(s)$ ，就是把$v_\pi(s_1),v_\pi(s_2),v_\pi(s_3)…$ 等所有的价值函数都要求出来。我们这节的目的就是把公式中的累加符号去掉，得到另一个更为简单的式子。接下来我们对下面的式子进行化简如下：<br>$$<br>\large{<br>\begin{align} </p>
<p>v_\pi(s) &#x3D;&amp; \sum_{a\in A} \pi(a\mid s) \sum_{s’,r}p(s’,r \mid s,a)[r+ \gamma v_\pi(s’)]\<br>&#x3D; &amp; \underbrace {\sum_{a\in A} \pi(a\mid s) \sum_{s’,r}p(s’,r \mid s,a) \cdot r }_{①}</p>
<p>+<br>\underbrace{\gamma \sum_{a\in A} \pi(a\mid s) \sum_{s’,r}p(s’,r \mid s,a) \cdot v_\pi(s’)}_{②}</p>
<p>\end{align}<br>}<br>$$<br><strong>分解①式</strong>：对于①式而言，$s’$ 只出现在函数$p(s’,r \mid s,a)$ 中，是可以积分积掉的。故由①得：<br>$$<br>\large{<br>\begin{align}</p>
<p>\sum_{a\in A} \pi(a\mid s) \sum_{s’,r}p(s’,r \mid s,a) \cdot r  \<br>&#x3D;&amp; \sum_{a\in A}  \pi(a\mid s) \underbrace {\sum_{r}  r\cdot p(r \mid s,a)}<em>{E_\pi[R</em>{t+1}  \mid S_t&#x3D;s,A_t&#x3D;a]} \quad\quad (1)  \</p>
<p>\end{align}<br>}<br>$$<br>（1）式正好是期望，此时我们也定义一个函数（记号） $r(s,a) \dot{&#x3D;} E_\pi[R_{t+1}  \mid S_t&#x3D;s,A_t&#x3D;a]$ 。表示$s,a$ 这个二元组的收益。故我们的推导变成如下：<br>$$<br>\large{<br>\begin{align}</p>
<p>\sum_{a\in A} \pi(a\mid s) \sum_{s’,r}p(s’,r \mid s,a) \cdot r  \<br>&#x3D;&amp; \sum_{a\in A} \pi(a\mid s) \underbrace {\sum_{r}  r\cdot p(r \mid s,a)}<em>{r(s,a) \dot{&#x3D;}E_\pi[R</em>{t+1}  \mid S_t&#x3D;s,A_t&#x3D;a]} \quad\quad (1)  \<br>&#x3D;&amp; \sum_{a\in A} \pi(a\mid s) \cdot r(s,a) \quad\quad\quad\quad \quad\quad \quad\quad\ (2)</p>
<p>\end{align}<br>}<br>$$<br>（2）式中，如果把 $a$ 都积分掉，那么（2）式就和$a$ 就没关系了，只和$s$ 有关，这里我们再次引入一个记号$r_\pi(s)$用来表示这个新的关系如下：<br>$$<br>\large{<br>\begin{align}</p>
<p>\sum_{a\in A} \pi(a\mid s) \sum_{s’,r}p(s’,r \mid s,a) \cdot r  \<br>&#x3D;&amp; \sum_{a\in A} \pi(a\mid s) \underbrace {\sum_{r}  r\cdot p(r \mid s,a)}<em>{r(s,a) \dot{&#x3D;}E_\pi[R</em>{t+1}  \mid S_t&#x3D;s,A_t&#x3D;a]} \quad\quad (1)  \<br>&#x3D;&amp; \sum_{a\in A} \pi(a\mid s) \cdot r(s,a) \quad\quad\quad\quad \quad\quad \quad\quad\ (2) \<br>\dot{&#x3D;}&amp;r_\pi(s)  \quad\quad\quad\quad \quad\quad \quad\quad\quad \quad\quad\ \quad\quad \quad\quad\ (3)</p>
<p>\end{align}<br>}<br>$$<br>此时，①式化简至此，需要提到的一点是，$r_\pi(s)$ 应该由多少个呢？在一开始我们就提到过 “$S$中有多少个$s$ 就会有多少$v_\pi(s)$，” ，显然 $r_\pi(s)$ 的数量也是 $\lvert S \rvert$ 个。故可得列向量如下:<br>$$<br>\large<br>r_\pi(s)&#x3D; \begin{pmatrix}<br>r_\pi(s_1)\<br>r_\pi(s_2)\<br>\vdots    \<br>r_\pi(s_{\lvert S \rvert})</p>
<p>\end{pmatrix} _{\lvert S \rvert \times 1}<br>$$<br><strong>分解②式</strong>：在 $p(s’,r \mid s,a) \cdot v_\pi(s’)$ 中，我们发现 $s’$ 在式子中都存在，但是$r$ 只存在于函数 $p(s’,r \mid s,a)$ 中，故积分可以积掉，则②式可以进一步写成：<br>$$<br>\large{<br>\begin{align}</p>
<p>\gamma \sum_{a\in A} \pi(a\mid s) \sum_{s’,r}p(s’,r \mid s,a) \cdot v_\pi(s’)<br>&#x3D;&amp; \gamma \sum_{a\in A} \pi(a\mid s) \sum_{s’}\underbrace{p(s’ \mid s,a)}_{状态转移函数} \cdot v_\pi(s’) \quad\quad (4)</p>
<p>\end{align}<br>}<br>$$<br>在（4）式中，可以发现在两个累加号中都有 $a$（注：虽然也都有 $s$，但是累加号下面是$a$ ，故积分只能积掉 $a$），积分可以积掉，但是一定和 $s,s’$ 有关，故引出一个记号 $P_\pi(s,s’)$ 来表示。 故继续推导如下：<br>$$<br>\large{<br>\begin{align}</p>
<p>\gamma \sum_{a\in A} \pi(a\mid s) \sum_{s’,r}p(s’,r \mid s,a) \cdot v_\pi(s’)<br>&#x3D;&amp; \gamma \sum_{a\in A} \pi(a\mid s) \sum_{s’}\underbrace{p(s’ \mid s,a)}_{状态转移函数} \cdot v_\pi(s’) \quad\quad (4) \</p>
<p>&#x3D;&amp; \gamma \sum_{s’} \underbrace{\sum_{a\in A} \pi(a\mid s) p(s’ \mid s,a)}_{P_\pi(s,s’)} \cdot v_\pi(s’) \quad\quad (5) \</p>
<p>&#x3D;&amp;  \gamma \sum_{s’} P_\pi(s,s’) \cdot v_\pi(s’) \quad\quad (6)\</p>
<p>\end{align}<br>}<br>$$<br>由（6）式可知，对于 $P_\pi(s,s’)$ 来说，一共由多少个值呢？很显然，$s$ 和 $s’$ 分别由 $\lvert S \rvert $ 个值，故 $P_\pi(s,s’)$ 会有 $\lvert S \rvert \times \lvert S \rvert  $  个。故可得矩阵如下：<br>$$<br>\large<br>P_\pi(s,s’)&#x3D; \begin{pmatrix}<br>P_\pi(s_1,s’<em>1) &amp; \cdots &amp; P_\pi(s_1,s’</em>{\lvert S \rvert})\<br>\vdots &amp; \ddots &amp; \vdots \<br>P_\pi(s_{\lvert S \rvert},s’<em>1) &amp; \cdots &amp; P_\pi(s</em>{\lvert S \rvert},s’_{\lvert S \rvert})\</p>
<p>\end{pmatrix} <em>{\lvert S \rvert \times \lvert S \rvert}<br>$$<br>于是，结合（3）和（6）式得：<br>$$<br>\large{<br>v_\pi(s) &#x3D; r_\pi(s) + \gamma \sum</em>{s’} P_\pi(s,s’) \cdot v_\pi(s’) \quad\quad\quad (7)<br>}<br>$$<br>令$s_i\dot{&#x3D;s},s_j \dot{&#x3D;} s’$ ，则（7） 式得：<br>$$<br>\large{<br>v_\pi(s) &#x3D; r_\pi(s) + \gamma \underbrace{\sum_{j&#x3D;1}^{\lvert S \rvert} P_\pi(s_i,s_j) }<em>{①}\cdot v_\pi(s’) \quad\quad\quad (7)<br>}<br>$$<br>由（7）式中的①式可得，正好是矩阵$P_\pi(s,s’)$ ，其中的 $P_\pi(s_i,s_j)$正是矩阵的一行。（7）式中$v_\pi(s)，r_\pi(s)，①，v_\pi(s’)$ 式（$v_\pi(s’)$本质是和$v_\pi(s)$是一样的），都分别对应一个矩阵，也即：<br>$$<br>\begin{pmatrix}<br>v_\pi(s_1)\<br>v_\pi(s_2)\<br>\vdots    \<br>v_\pi(s</em>{\lvert S \rvert})<br>\end{pmatrix} _{\lvert S \rvert \times 1}<br>&#x3D;</p>
<p>\begin{pmatrix}<br>r_\pi(s_1)\<br>r_\pi(s_2)\<br>\vdots    \<br>r_\pi(s_{\lvert S \rvert})<br>\end{pmatrix} _{\lvert S \rvert \times 1}</p>
<ul>
<li>\gamma</li>
</ul>
<p>\begin{pmatrix}<br>P_\pi(s_1,s’<em>1) &amp; \cdots &amp; P_\pi(s_1,s’</em>{\lvert S \rvert})\<br>\vdots &amp; \ddots &amp; \vdots \<br>P_\pi(s_{\lvert S \rvert},s’<em>1) &amp; \cdots &amp; P_\pi(s</em>{\lvert S \rvert},s’_{\lvert S \rvert})\<br>\end{pmatrix} _{\lvert S \rvert \times \lvert S \rvert}<br>\cdot </p>
<p>\begin{pmatrix}<br>v_\pi(s_1)\<br>v_\pi(s_2)\<br>\vdots    \<br>v_\pi(s_{\lvert S \rvert})<br>\end{pmatrix} _{\lvert S \rvert \times 1}<br>$$<br>最终化简到矩阵的运算。故，由（7）式进一步得：<br>$$<br>\large{<br>\begin{align}<br>v_\pi(s) &#x3D;&amp; r_\pi(s) + \gamma P_\pi(s,s’) \cdot v_\pi(s’) \quad\quad\quad \<br>v_\pi&#x3D;&amp; r_\pi + \gamma P_\pi \cdot v_\pi \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\qquad (8)\<br>(I-P_\pi)v_\pi &#x3D;&amp; r_\pi \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\qquad\quad\quad\quad\quad\quad\ (9) \<br>v_\pi &#x3D;&amp; (I-P_\pi)^{-1} r_\pi \qquad\qquad\qquad\qquad\qquad\qquad(10)<br>\end{align}<br>}<br>$$<br>经过以上推导，得出（10）式。其中在（8）式中$v_\pi(s’)$ 其实是 $v_\pi(s)$ 的迭代，所以直接用 $v_\pi(s)$ 替换，在（9）式中$\gamma$ 是个常数，也可忽略。从向量中可以得出复杂度$O({\lvert S \rvert}^3)$</p>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20201203135011156.png" alt="image-20201203135011156"></p>
<h3 id="策略评估-迭代解"><a href="#策略评估-迭代解" class="headerlink" title="策略评估-迭代解"></a>策略评估-迭代解</h3><p>首先，我们来看如何使用动态规划来求解强化学习的<strong>预测问题</strong>，即求解给定策略的状态价值函数的问题。这个问题的求解过程我们通常叫做策略评估(Policy Evaluation)。</p>
<p>策略评估的基本思路是从<strong>任意一个状态价值函数开始</strong>，依据给定的策略，结合贝尔曼期望方程、状态转移概率和奖励同步迭代更新状态价值函数，直至其<strong>收敛</strong>，得到该策略下最终的状态价值函数。</p>
<p>假设我们在第$k$轮迭代已经计算出了所有的状态的状态价值，那么在第$k+1$轮我们可以利用第$k$轮计算出的<strong>状态价值</strong>计算出第$k+1$轮的状态价值。这是通过贝尔曼方程来完成的，即：<br>$$<br>\large{<br>v_{k+1}(s) &#x3D; \sum\limits_{a \in A} \pi(a|s)(R_s^a + \gamma \sum\limits_{s’ \in S}P_{ss’}^av_{k}(s’))<br> \</p>
<p> v_{k+1}(s) \dot{&#x3D;}\sum_{a\in A} \pi(a\mid s) \sum_{s’,r}p(s’,r \mid s,a)[r+ \gamma v_k(s’)]<br>}<br>$$<br>和上一节的式子唯一的区别是由于我们的策略$\pi$已经给定，我们不再写出，对应加上了迭代轮数的下标。我们每一轮可以对计算得到的新的状态价值函数再次进行迭代，直至状态价值的值改变**很小(收敛)**，那么我们就得出了预测问题的解，即给定策略的状态价值函数$v_\pi$。</p>
<h3 id="Summary-1"><a href="#Summary-1" class="headerlink" title="Summary"></a>Summary</h3><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20201203183436771.png" alt="image-20201203183436771"></p>
<h3 id="策略评估-（迭代解）应用"><a href="#策略评估-（迭代解）应用" class="headerlink" title="策略评估-（迭代解）应用"></a>策略评估-（迭代解）应用</h3><p>下面我们用一个具体的例子来说明策略评估的过程。</p>
<p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20201203142402216.png" alt="image-20201203142402216"></p>
<p>这是一个经典的Grid World的例子。我们有一个<code>4x4</code>的16宫格。只有左上和右下的格子是终止格子。该位置的价值固定为0，个体如果到达了该2个格子，则停止移动，此后每轮奖励都是0。个体在16宫格其他格的每次移动，得到的即时奖励R都是-1。注意个体每次只能移动一个格子，且只能上下左右4种移动选择，不能斜着走, 如果在边界格往外走，则会直接移动回到之前的边界格。衰减因子我们定义为$\gamma &#x3D;1$。由于这里每次移动，下一格都是固定的，因此所有可行的的状态转化概率$P &#x3D;1$。这里给定的策略是随机策略，即每个格子里有25%的概率向周围的4个格子移动。</p>
<p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20201203142716568.png" alt="image-20201203142716568"></p>
<p>首先我们初始化所有格子的状态价值为0，如上图$k&#x3D;0$的时候。现在我们开始策略迭代了。由于终止格子的价值固定为0，我们可以不将其加入迭代过程。</p>
<p><strong>在$k&#x3D;1$时</strong>，我们利用上面的贝尔曼方程先计算第二行第一个格子的价值：<br>$$<br>v_1^{(21)} &#x3D; \frac{1}{4}[(-1+0) +(-1+0)+(-1+0)+(-1+0)] &#x3D; -1<br>$$<br>第二行第二个格子的价值是：<br>$$<br>v_1^{(22)} &#x3D; \frac{1}{4}[(-1+0) +(-1+0)+(-1+0)+(-1+0)] &#x3D; -1<br>$$<br>其他的格子都是类似的，第一轮的状态价值迭代的结果如上图$k&#x3D;1$的时候。现在我们第一轮迭代完了。</p>
<p><strong>在$k&#x3D;1$时</strong>，还是看第二行第一个格子的价值：<br>$$<br>v_2^{(21)} &#x3D; \frac{1}{4}[(-1+0) +(-1-1)+(-1-1)+(-1-1)] &#x3D; -1.75<br>$$<br>第二行第二个格子的价值是：<br>$$<br>v_2^{(22)} &#x3D; \frac{1}{4}[(-1-1) +(-1-1)+(-1-1)+(-1-1)] &#x3D; -2<br>$$<br>最终得到的结果是上图$k&#x3D;2$的时候，第二轮迭代完毕。</p>
<p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20201203142745214.png" alt="image-20201203142745214"></p>
<p><strong>在$k&#x3D;3$时</strong>：<br>$$<br>v_3^{(21)} &#x3D; \frac{1}{4}[(-1+0)+(-1-1.7) +(-1-2)+(-1-2)] &#x3D; -2.425 \<br>v_3^{(22)} &#x3D; \frac{1}{4}[(-1-1.7) +(-1-1.7)+(-1-2)+(-1-2)] &#x3D; -2.85<br>$$</p>
<blockquote>
<p>计算加和的过程 就是 上、下、左、右四个方向，其中无论哪次迭代，都有 $v_k^{11} &#x3D; 0$。</p>
</blockquote>
<p>最终得到的结果是上图$k&#x3D;3$的时候。就这样一直迭代下去，直到每个格子的策略价值改变很小（收敛）为止。这时我们就得到了所有格子的基于随机策略的状态价值。</p>
<p>可以看到，动态规划的策略评估计算过程并不复杂，但是如果我们的问题是一个非常复杂的模型的话，这个计算量还是非常大的。</p>
<h3 id="策略改进-策略改进定理"><a href="#策略改进-策略改进定理" class="headerlink" title="策略改进-策略改进定理"></a>策略改进-策略改进定理</h3><p>现在出现了这样一个问题，如果给定一个 $\pi$ 和 $\pi’$ ，我们如何判断哪一策略更好呢？采用我们以前提到的方法，就是分别计算各自对应的价值函数，然后通过判断两个价值函数的大小来判断策略的好坏。虽然能够得出结论，但是这个计算的过程也是会占用 “资源”的，能否有另外一种方式可以实现相应的功能呢？有，那就是<strong>策略改进定理。</strong></p>
<p><strong>策略改进定理：</strong> <strong>给定$\pi，\pi’$，如果$\forall(s) \in S,q_\pi(s,\pi’(s)) \geq v_\pi(s)$，那么，则有 $\forall s\in S,v_{\pi’}(s) \geq v_\pi(s)$。</strong></p>
<p>通过上面的定理可得，$q_\pi(s,a)$ 只要算出来了，那么 $v_\pi(s)$ 自然也会得出（$v_\pi(s)$ 是$q_\pi(s,a)$的加权平均），此时我们不再需要再求得 $v_{\pi’}(s)$，而是直接把$\pi’(s)$ 带入到 $q_\pi(s,a)$ 中的a中即可。</p>
<p>下面进行定理的一个证明：</p>
<p>在 $q_\pi(s,\pi’(s)) \geq v_\pi(s)$ 中，里面的 $q_\pi(s,a)$ 和 $v_\pi(s’)$ 有如下关系，我们在第一张MDP已经证明过：<br>$$<br>\large{<br>\begin{align}<br>q_\pi(s,a) &#x3D;&amp; \sum_{s’,r}P(s’,r \mid s,a)[r+ \gamma v_\pi(s’)] \<br>&#x3D;&amp;E_\pi[R_{t+1} + \gamma v_\pi(s_{t+1}) \mid S_t &#x3D; s, A_t&#x3D;a]<br>\end{align}<br>}<br>$$<br>其实，这里需要说明一点，上式中 $E_\pi$ 中的$\pi$ 严格意义上是不能带着的，对于$R_{t+1}$ ， 不是$\pi$ 控制的，详情看下图的蓝色虚线（在第一章中，我们称其未系统之间的状态转移），</p>
<p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/hss02.svg" alt="image-20201128135658979"></p>
<p>当然了，对于 $R_{t+2}, R_{t+3}$ 等是需要 $\pi$ 控制的，在公式中的体现就是$v_\pi$。故，这里准确写法应该如下：<br>$$<br>\large{<br>\begin{align}<br>q_\pi(s,a) &#x3D;&amp; \sum_{s’,r}P(s’,r \mid s,a)[r+ \gamma v_\pi(s’)] \<br>&#x3D;&amp; E[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t &#x3D; s, A_t&#x3D;a]<br>\end{align}<br>}<br>$$<br><strong>证明：</strong>$\forall(s) \in S,q_\pi(s,\pi’(s)) \geq v_\pi(s)$，把 $a&#x3D;\pi’(s)$ 带入得：<br>$$<br>\large{<br>\begin{align}<br>v_\pi(s) \leq &amp; q_\pi(s,\pi’(s)) \<br>&#x3D;&amp; E[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t &#x3D; s, A_t&#x3D;\pi’(s)] \quad\quad\quad(1) \<br>&#x3D;&amp; E_{\pi’}[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t &#x3D; s] \quad\quad\quad\quad\quad\quad\quad\quad (2)</p>
<p>\end{align}<br>}<br>$$<br>上面式子中，在（2）式中，我们把 $\pi’$那个策略定义到 $R_{t+1}$ 上，其余的还是采取 $\pi$ 的策略。在（2）式中出现了 $v_\pi(s_{t+1})$ ，再带入$v_\pi(s) \leq q_\pi(s,\pi’(s))$  得：<br>$$<br>\large{<br>\begin{align}<br>v_\pi(s) \leq &amp; q_\pi(s,\pi’(s)) \<br>&#x3D;&amp; E[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t &#x3D; s, A_t&#x3D;\pi’(s)] \quad\quad\quad(1) \<br>&#x3D;&amp; E_{\pi’}[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t &#x3D; s] \quad\quad\quad\quad\quad\quad\quad\quad\ (2) \</p>
<p>\leq &amp; E_{\pi’}[R_{t+1} + \gamma q_\pi(S_{t+1},\pi’(S_{t+1})) \mid S_t &#x3D; s] \quad\quad\quad\quad\quad\ (3)</p>
<p>\end{align}<br>}<br>$$<br>再把$q_\pi(s_{t+1},\pi’(s_{t+1}))$ 带入（2）式得：<br>$$<br>\large{<br>\begin{align}<br>v_\pi(s) \leq &amp; q_\pi(s,\pi’(s)) \<br>&#x3D;&amp; E[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t &#x3D; s, A_t&#x3D;\pi’(s)] \quad\quad\quad(1) \<br>&#x3D;&amp; E_{\pi’}[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t &#x3D; s] \quad\quad\quad\quad\quad\quad\quad\quad\ (2) \</p>
<p>\leq &amp; E_{\pi’}[R_{t+1} + \gamma q_\pi(S_{t+1},\pi’(S_{t+1})) \mid S_t &#x3D; s] \quad\quad\quad\quad\quad\ (3) \</p>
<p>&#x3D;&amp;E_{\pi’}[R_{t+1} + \gamma E_{\pi’}[R_{t+2} + \gamma v_\pi(S_{t+2}) \mid S_{t+1}] \mid S_t&#x3D;s ]\quad\quad\ (4)</p>
<p>\end{align}<br>}<br>$$<br>有一点注意，在（4）式中 $S_{t+1}$ 没有写成等于多少的形式，这里只是对于$S_{t+2}$ 的前提条件，表明其不能独立出现。然后再对（4）展开得：<br>$$<br>\large{<br>\begin{align}<br>v_\pi(s) \leq &amp; q_\pi(s,\pi’(s)) \<br>&#x3D;&amp; E[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t &#x3D; s, A_t&#x3D;\pi’(s)] \quad\quad\quad(1) \<br>&#x3D;&amp; E_{\pi’}[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t &#x3D; s] \quad\quad\quad\quad\quad\quad\quad\quad\ (2) \</p>
<p>\leq &amp; E_{\pi’}[R_{t+1} + \gamma q_\pi(S_{t+1},\pi’(S_{t+1})) \mid S_t &#x3D; s] \quad\quad\quad\quad\quad\ (3) \</p>
<p>&#x3D;&amp;E_{\pi’}[R_{t+1} + \gamma E_{\pi’}[R_{t+2} + \gamma v_\pi(S_{t+2}) \mid S_{t+1}] \mid S_t&#x3D;s ]\quad\quad\ (4) \</p>
<p>&#x3D;&amp;E_{\pi’}[R_{t+1} + \gamma E_{\pi’}[R_{t+2} \mid S_{t+1}] + \gamma^2 E_{\pi’}[v_\pi(S_{t+2}) \mid S_{t+1}] \mid S_t&#x3D;s ]\quad\quad\ (5)</p>
<p>\end{align}<br>}<br>$$<br>对于（5）式，出现了“期望套期望”，期望的期望还是期望，并且都是 $E{\pi’}$，故（5）式可以进一步化简得：<br>$$<br>\large{<br>\begin{align}<br>v_\pi(s) \leq &amp; q_\pi(s,\pi’(s)) \<br>&#x3D;&amp; E[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t &#x3D; s, A_t&#x3D;\pi’(s)] \quad\quad\quad(1) \<br>&#x3D;&amp; E_{\pi’}[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t &#x3D; s] \quad\quad\quad\quad\quad\quad\quad\quad\ (2) \</p>
<p>\leq &amp; E_{\pi’}[R_{t+1} + \gamma q_\pi(S_{t+1},\pi’(S_{t+1})) \mid S_t &#x3D; s] \quad\quad\quad\quad\quad\ (3) \</p>
<p>&#x3D;&amp;E_{\pi’}[R_{t+1} + \gamma E_{\pi’}[R_{t+2} + \gamma v_\pi(S_{t+2}) \mid S_{t+1}] \mid S_t&#x3D;s ]\quad\quad\ (4) \</p>
<p>&#x3D;&amp;E_{\pi’}[R_{t+1} + \gamma E_{\pi’}[R_{t+2} \mid S_{t+1}] + \gamma^2 E_{\pi’}[v_\pi(S_{t+2}) \mid S_{t+1}] \mid S_t&#x3D;s ]\quad\quad\ (5) \</p>
<p>&#x3D;&amp;E_{\pi’}[R_{t+1} + \gamma R_{t+2}  + \gamma^2v_\pi(S_{t+2}) \mid S_t&#x3D;s ]\quad\quad\ (6)</p>
<p>\end{align}<br>}<br>$$<br>到这里，只走了两步，可以看到，由 $\pi’$ 一开始的只控制 $R_{t+1}$ ，走完两步后，$R_{t+2}$ 也属于 $\pi’$ 控制，接下来继续走的话，$\pi’$ 控制的 $R$ 会越来越多，$\pi$ 控制的 $R$ 越来越少。<br>$$<br>\large{<br>\begin{align}<br>v_\pi(s) \leq &amp; q_\pi(s,\pi’(s)) \<br>&#x3D;&amp; E[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t &#x3D; s, A_t&#x3D;\pi’(s)] \quad\quad\quad(1) \<br>&#x3D;&amp; E_{\pi’}[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t &#x3D; s] \quad\quad\quad\quad\quad\quad\quad\quad\ (2) \</p>
<p>\leq &amp; E_{\pi’}[R_{t+1} + \gamma q_\pi(S_{t+1},\pi’(S_{t+1})) \mid S_t &#x3D; s] \quad\quad\quad\quad\quad\ (3) \</p>
<p>&#x3D;&amp; E_{\pi’}[R_{t+1} + \gamma E_{\pi’}[R_{t+2} + \gamma v_\pi(S_{t+2}) \mid S_{t+1}] \mid S_t&#x3D;s ]\quad\quad\ (4) \</p>
<p>&#x3D;&amp; E_{\pi’}[R_{t+1} + \gamma E_{\pi’}[R_{t+2} \mid S_{t+1}] + \gamma^2 E_{\pi’}[v_\pi(S_{t+2}) \mid S_{t+1}] \mid S_t&#x3D;s ]\quad\quad\ (5) \</p>
<p>&#x3D;&amp; E_{\pi’}[R_{t+1} + \gamma R_{t+2}  + \gamma^2v_\pi(S_{t+2}) \mid S_t&#x3D;s ]\quad\quad\ (6)\</p>
<p>\leq &amp; E_{\pi’}[R_{t+1} + \gamma R_{t+2}  +\gamma^2 R_{t+3} + \gamma^3v_\pi(S_{t+3}) \mid S_t&#x3D;s ]\quad\quad\quad (7)\</p>
<p>\vdots \</p>
<p>\leq &amp; E_{\pi’} \underbrace{[R_{t+1} + \gamma R_{t+2}  +\gamma^2 R_{t+3} + \gamma^4 R_{t+4} + \cdots}_{G_t} \mid S_t&#x3D;s ] \quad\quad\ (8)\ </p>
<p>&#x3D;&amp; v_{\pi’}(s)</p>
<p>\end{align}<br>}<br>$$<br>故得到：$v_\pi’(s) \geq v_\pi(s)$， 得证。</p>
<h3 id="Summary-2"><a href="#Summary-2" class="headerlink" title="Summary"></a>Summary</h3><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20201203210853298.png" alt="image-20201203210853298"></p>
<h3 id="策略改进-贪心策略"><a href="#策略改进-贪心策略" class="headerlink" title="策略改进-贪心策略"></a>策略改进-贪心策略</h3><p>这节就是利用策略改进定理提出一种策略改进的方法——<strong>贪心策略</strong>（Greedy Policy）。对于 $\forall s \in S$，定义如下公式：<br>$$<br>\large{<br>\pi’(s) &#x3D; \underset{a}{argmax}\ q_\pi(s,a) \quad\quad (1)<br>}<br>$$<br>上面式子的意识是说，根据我们上节课所说的，从一个策略$\pi$，经过策略评估得到一些 $\pi’$ ，如下图所示。并从这些 $\pi’$ 中选择一个最大的$q_\pi(s,a)$。$\underset{a}{argmax}$ 表示能够使得表达式的值最大化的 $a$。</p>
<p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/pe.svg"></p>
<p>由在第一章讲到的 $v_\pi(s)$ 和 $q_\pi(s,a)$ 的 关系得：$v_\pi(s) \leq \underset{a}max \ q_\pi(s,a)$ 。又因由（1）式可得， $q_\pi(s,\pi’(s)) &#x3D; \underset{a}max \ q_\pi(s,a)$ 故得：<br>$$<br>\large{<br>v_\pi(s) \leq \underset{a}max \ q_\pi(s,a) &#x3D;q_\pi(s,\pi’(s)) \<br>v_\pi(s) \leq q_\pi(s,\pi’(s)) \quad\quad\quad \quad \qquad\quad\quad \qquad\quad\quad<br>(2)<br>}<br>$$<br>（2）式正好满足上节我们说的策略改进定理。<strong>故由策略定理得知：对于 $\forall s \in S$，$v_{\pi’}(s) \geq v_\pi(s)$。</strong></p>
<p>如果在某一个时刻，一直迭代，如果出现了 $ v_\pi(s)&#x3D;v_{\pi’}(s)$ ，这种情况，也就是说明 $v_\pi(s)$ 此时已经不能再好了，并且此时$ v_\pi(s)&#x3D;v_{\pi’}(s) &#x3D;v_*$。如下图所示：</p>
<p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/veq.svg"></p>
<blockquote>
<p><strong>证明</strong>：如果$v_{\pi’}&#x3D;v_\pi$ 那么，$ v_\pi(s)&#x3D;v_{\pi’}(s) &#x3D;v_*$</p>
</blockquote>
<p>证：由 $v_{\pi’}&#x3D;v_\pi$， 可以得出 $q_{\pi’}&#x3D;q_\pi。$$\forall s\in S$，由$v_\pi(s) &#x3D; \sum_{a\in A} \pi(a\mid s) ·q_\pi(s,a) $可得：<br>$$<br>\large{<br>\begin{align}<br>v_{\pi’}(s) &#x3D;&amp; \sum_{a\in A} {\pi’}(a\mid s) ·q_{\pi’}(s,a)\<br>&#x3D;&amp; \sum_{a\in A} {\pi’}(a\mid s) ·q_{\pi}(s,a) \quad\quad\quad\ (3) \  </p>
<p>\end{align}<br>}<br>$$<br>因为前面说的 $\pi’$ 都是选择一个最优的策略，也就是<strong>确定性策略</strong>，如下图所示：</p>
<p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/hss02.svg" alt="image-20201128135658979"></p>
<p>假如我们选择<code>a3</code>这个策略为 $\pi’$ 。故在（3）式中的加和可以去掉了，因为是确定性策略，那么选择<code>a2</code> 和 <code>a1</code>的概率就是0，例缩当然加和后只剩下<code>a3</code> 这个策略$\pi’$，故继续推导得：<br>$$<br>\large{<br>\begin{align}<br>v_{\pi’}(s) &#x3D;&amp; \sum_{a\in A} {\pi’}(a\mid s) ·q_{\pi’}(s,a)\<br>&#x3D;&amp; \sum_{a\in A} {\pi’}(a\mid s) ·q_{\pi}(s,a) \quad\quad\quad\ (3) \<br>&#x3D;&amp; q_\pi(s,\pi’(s)) \quad\quad\quad\quad\quad\quad\quad\quad\ (4)  \</p>
<p>\end{align}<br>}<br>$$<br>由（4）式和（2）中$\underset{a}max \ q_\pi(s,a) &#x3D;q_\pi(s,\pi’(s))$ 得：<br>$$<br>\large{<br>\begin{align}<br>v_{\pi’}(s) &#x3D;&amp; \sum_{a\in A} {\pi’}(a\mid s) ·q_{\pi’}(s,a)\<br>&#x3D;&amp; \sum_{a\in A} {\pi’}(a\mid s) ·q_{\pi}(s,a) \quad\quad\quad\ (3) \<br>&#x3D;&amp; q_\pi(s,\pi’(s)) \quad\quad\quad\quad\quad\quad\quad\quad\quad (4)  \<br>&#x3D;&amp; \underset{a}max \ q_\pi(s,a) \qquad\quad\quad\quad\quad\quad\quad\ (5)\</p>
<p>\end{align}<br>}<br>$$<br>（5）式再由$q_\pi(s,a) &#x3D;\sum_{s’,r}P(s’,r \mid s,a)[r+ \gamma v_\pi(s’)] $ 可得：<br>$$<br>\large{<br>\begin{align}<br>v_{\pi’}(s) &#x3D;&amp; \sum_{a\in A} {\pi’}(a\mid s) ·q_{\pi’}(s,a)\<br>&#x3D;&amp; \sum_{a\in A} {\pi’}(a\mid s) ·q_{\pi}(s,a) \quad\quad\quad\ (3) \<br>&#x3D;&amp; q_\pi(s,\pi’(s)) \quad\quad\quad\quad\quad\quad\quad\quad\quad (4)  \<br>&#x3D;&amp; \underset{a}max \ q_\pi(s,a) \qquad\quad\quad\quad\quad\quad\quad\ (5)\<br>&#x3D;&amp; \underset{a}max \sum_{s’,r}P(s’,r \mid s,a)[r+ \gamma v_\pi(s’)] \quad (6) \<br>\end{align}<br>}<br>$$<br>在（6）式中，又因为题设中，$v_{\pi’}&#x3D;v_\pi$ 故带入得：<br>$$<br>\large{<br>\begin{align}<br>v_{\pi’}(s) &#x3D;&amp; \sum_{a\in A} {\pi’}(a\mid s) ·q_{\pi’}(s,a)\<br>&#x3D;&amp; \sum_{a\in A} {\pi’}(a\mid s) ·q_{\pi}(s,a) \quad\quad\quad\ (3) \<br>&#x3D;&amp; q_\pi(s,\pi’(s)) \quad\quad\quad\quad\quad\quad\quad\quad\quad (4)  \<br>&#x3D;&amp; \underset{a}max \ q_\pi(s,a) \qquad\quad\quad\quad\quad\quad\quad\ (5)\<br>&#x3D;&amp; \underset{a}max \sum_{s’,r}P(s’,r \mid s,a)[r+ \gamma v_\pi(s’)] \quad (6) \<br>&#x3D;&amp; \underset{a}max \sum_{s’,r}P(s’,r \mid s,a)[r+ \gamma v_{\pi’}(s’)] \quad (7)<br>\end{align}<br>}<br>$$<br>故此时我们得到：<br>$$<br>\large<br>v_{\pi’}(s) &#x3D; \underset{a}max \sum_{s’,r}P(s’,r \mid s,a)[r+ \gamma v_{\pi’}(s’)] \quad(8)<br>$$<br>并且我们由贝尔曼最优方程得：<br>$$<br>\large{<br>v_*(s)&#x3D;\underset{a}{max}\sum_{s’,r}P(s’,r \mid s,a)[r+\gamma v_\pi(s’)] \quad\quad(9) \</p>
<p>}<br>$$<br>故由（8）式和（9）式，以及题设$v_{\pi’}&#x3D;v_\pi$ 得证：<br>$$<br> \large<br> v_\pi(s)&#x3D;v_{\pi’}(s) &#x3D;v_*<br>$$</p>
<h3 id="Summary-3"><a href="#Summary-3" class="headerlink" title="Summary"></a>Summary</h3><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20201205145118572.png" alt="image-20201205145118572"></p>
<h2 id="价值迭代"><a href="#价值迭代" class="headerlink" title="价值迭代"></a>价值迭代</h2><h3 id="策略迭代的缺点"><a href="#策略迭代的缺点" class="headerlink" title="策略迭代的缺点"></a>策略迭代的缺点</h3><p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20201205150228362.png" alt="image-20201205150228362"></p>
<p>从上图可以发现，在策略迭代中， 其实进行了两次策略循环，第一层是在策略评估中，第二层是策略迭代这一层。故，如果迭代次数过多的化，其实是很低效率的。<strong>策略评估的一个缺点是每一次迭代过程中都涉及了策略评估。</strong></p>
<h3 id="价值迭代-1"><a href="#价值迭代-1" class="headerlink" title="价值迭代"></a>价值迭代</h3><p>根据以上， 这里我们讨论一种极端的情况，只计算第一次的价值函数 $v_1(s)$， 从此进行截断，后续不在计算，也即是说，只对 $v_1(s)$ 进行策略评估，并把这个算法称为<strong>价值迭代</strong>。如图：</p>
<p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/jieduan1.svg"></p>
<p>如下图所示，价值迭代只计算 从状态$s$ 到$s’$的一条分支（$v_1(s)$），假定以最右侧分支为例，这是策略评估的<strong>一步</strong>，加上策略改进后，其实只走了“半步”，也就是从$s$ 状态走到a3（假定选择了action a3），此时利用策略改进算法即可用a3处的$q_\pi(s,a)$ ，并得到$v_\pi(s)$ 和 $v_{\pi’}(s)$的大小关系，以下图为例，假设a3为$v_\pi(s)$，则a2或者a1就是$v_{\pi’}(s)$。</p>
<p>也即：如果按照策略迭代的方法，需要计算完所有v1（选择a1的$v_\pi(s)-&gt;v_{\pi’}(s)$），v2，v3，如上图所示。但是依据价值迭代 + 策略改进，只需要计算v1的“半步”即可，因为根据策略改进定理，只需要计算出$q_\pi(s,a)$，即可得出$v_\pi(s)$ 和 $v_{\pi’}(s)$的大小关系。</p>
<p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/hss02.svg" alt="image-20201128135658979"></p>
<p>从 <strong>策略评估-（迭代解）应用</strong>小节 中可以知道，这个会有多个v1（注意：v1指的是多个状态，比如$v1^{21}$是第一次迭代中第二行第一个格子的状态，这里的“1”其实指的是迭代次数，并不一个状态）, 在这里选一个最大$v_\pi$作为$v*$ 。即完成了一次迭代。下面的公式，其是正是贝尔曼最优方程（求$v*$ ，$q*$）的 求$v*$的过程，同时也是价值迭代的算法公式：<br>$$<br>\large{<br> v_{k+1}(s) \dot{&#x3D;}\underset{a}{max} \sum_{s’,r}p(s’,r \mid s,a)[r+ \gamma v_k(s’) \quad\quad(1)<br>}<br>$$<br>从上面（1）式中，从贝尔曼最优方程的较多来说，这里变成<strong>了一条更新规则</strong>，还是以<strong>策略评估-（迭代解）应用</strong>小节的例子举例，只计算v1，把“每个格子”的数据更新了一次（第一次迭代得到v1），而不需要在根据v1的数据j计算v2，再进行更新了。</p>
<p>除此之外，可以把（1）式和 “<strong>策略评估-迭代解</strong>”的更新策略$ v_{k+1}(s) \dot{&#x3D;}\sum_{a\in A} \pi(a\mid s) \sum_{s’,r}p(s’,r \mid s,a)[r+ \gamma v_k(s’)]$比较，唯一不同的是，在策略评估-迭代解中，对于 $s$状态的下一步<code>action</code> 需要根据概率求得，而在价值迭代中，直接在v1中选择最大的<code>action</code>，故概率此时为1。即：<strong>价值迭代是极端情况下的策略迭代</strong>。</p>
<p>根据上面描述的极端情况，其实就是价值迭代，总结如下：</p>
<p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20201205213726436.png" alt="image-20201205213726436"></p>
<h3 id="异步动态规划"><a href="#异步动态规划" class="headerlink" title="异步动态规划"></a>异步动态规划</h3><p>异步动态规划，也叫就地迭代，它是基于价值迭代的一种算法。在样本空间比较大的情况下，即使只进行一次迭代，也会花费很长的时间，还是以<strong>策略评估-（迭代解）应用</strong> 小节的例子，比如下图：</p>
<p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20201205220225280.png" alt="image-20201205220225280"></p>
<p>如果，格子的数量及其多，即使只计算v1，（更新一次），迭代一次，花费时间也是巨大的。所以就出现了一种算法，这种算法，只随机找到“其中的一个格子” 进行更新，这就是<strong>异步动态规划</strong>。然而，为了收敛，异步算法必须不断的更新所有状态的值。</p>
<h2 id="广义策略迭代"><a href="#广义策略迭代" class="headerlink" title="广义策略迭代"></a>广义策略迭代</h2><p>广义策略迭代，其实就包含了前面所说的一般的策略迭代，价值迭代，还有就地迭代。如下面的图，如果都走到“顶”，那就是一般的策略迭代。如果走不到“顶”就可能是其他的迭代。</p>
<p><img src="https://raw.githubusercontent.com/zhaohongqiangsoliva/zhaohongqiangsoliva.github.io/master/_posts/img/image-20201205221614255.png" alt="image-20201205221614255"></p>
<p>按照下面的例子和类比，就很容易理解了，可以把这个广义策略迭代类比买房：</p>
<p><strong>策略迭代</strong>：全款买房，以旧换新。</p>
<p><strong>价值迭代</strong>：首付，以旧换新。</p>
<p><strong>就地策略迭代</strong>：几乎0首付，以旧换新。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/pinard/p/9463815.html">https://www.cnblogs.com/pinard/p/9463815.html</a></p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1nV411k7ve?t=1738">https://www.bilibili.com/video/BV1nV411k7ve?t=1738</a></p>
<p><a target="_blank" rel="noopener" href="https://www.davidsilver.uk/wp-content/uploads/2020/03/DP.pdf">https://www.davidsilver.uk/wp-content/uploads/2020/03/DP.pdf</a></p>
<p><a target="_blank" rel="noopener" href="http://www.incompleteideas.net/book/the-book.html">http://www.incompleteideas.net/book/the-book.html</a></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="http://zhaohongqiangsoliva.github.io/public">John Doe</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://zhaohongqiangsoliva.github.io/public/2022/08/16/2021-10-02-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-02-DP/">http://zhaohongqiangsoliva.github.io/public/2022/08/16/2021-10-02-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-02-DP/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/public/2022/08/16/2021-10-02-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-03-MCMC/"><img class="prev-cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="onerror=null;src='/public/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info"></div></div></a></div><div class="next-post pull-right"><a href="/public/2022/08/16/2021-10-02-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-01-MDPs/"><img class="next-cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="onerror=null;src='/public/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info"></div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="this.onerror=null;this.src='/public/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">John Doe</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/public/archives/"><div class="headline">Articles</div><div class="length-num">5</div></a><a href="/public/tags/"><div class="headline">Tags</div><div class="length-num">0</div></a><a href="/public/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92-DP"><span class="toc-number">1.</span> <span class="toc-text">强化学习-动态规划-DP</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3"><span class="toc-number">1.1.</span> <span class="toc-text">策略迭代</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AD%96%E7%95%A5%E8%AF%84%E4%BC%B0-%E8%A7%A3%E6%9E%90%E8%A7%A3"><span class="toc-number">1.1.1.</span> <span class="toc-text">策略评估-解析解</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Summary"><span class="toc-number">1.1.2.</span> <span class="toc-text">Summary</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AD%96%E7%95%A5%E8%AF%84%E4%BC%B0-%E8%BF%AD%E4%BB%A3%E8%A7%A3"><span class="toc-number">1.1.3.</span> <span class="toc-text">策略评估-迭代解</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Summary-1"><span class="toc-number">1.1.4.</span> <span class="toc-text">Summary</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AD%96%E7%95%A5%E8%AF%84%E4%BC%B0-%EF%BC%88%E8%BF%AD%E4%BB%A3%E8%A7%A3%EF%BC%89%E5%BA%94%E7%94%A8"><span class="toc-number">1.1.5.</span> <span class="toc-text">策略评估-（迭代解）应用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AD%96%E7%95%A5%E6%94%B9%E8%BF%9B-%E7%AD%96%E7%95%A5%E6%94%B9%E8%BF%9B%E5%AE%9A%E7%90%86"><span class="toc-number">1.1.6.</span> <span class="toc-text">策略改进-策略改进定理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Summary-2"><span class="toc-number">1.1.7.</span> <span class="toc-text">Summary</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AD%96%E7%95%A5%E6%94%B9%E8%BF%9B-%E8%B4%AA%E5%BF%83%E7%AD%96%E7%95%A5"><span class="toc-number">1.1.8.</span> <span class="toc-text">策略改进-贪心策略</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Summary-3"><span class="toc-number">1.1.9.</span> <span class="toc-text">Summary</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%B7%E5%80%BC%E8%BF%AD%E4%BB%A3"><span class="toc-number">1.2.</span> <span class="toc-text">价值迭代</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3%E7%9A%84%E7%BC%BA%E7%82%B9"><span class="toc-number">1.2.1.</span> <span class="toc-text">策略迭代的缺点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%B7%E5%80%BC%E8%BF%AD%E4%BB%A3-1"><span class="toc-number">1.2.2.</span> <span class="toc-text">价值迭代</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%82%E6%AD%A5%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92"><span class="toc-number">1.2.3.</span> <span class="toc-text">异步动态规划</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B9%BF%E4%B9%89%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3"><span class="toc-number">1.3.</span> <span class="toc-text">广义策略迭代</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"><span class="toc-number">1.4.</span> <span class="toc-text">参考文献</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/public/2022/08/16/2021-10-02-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-04-%E5%85%8D%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E9%A2%84%E6%B5%8B/" title="No title"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/public/img/404.jpg'" alt="No title"/></a><div class="content"><a class="title" href="/public/2022/08/16/2021-10-02-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-04-%E5%85%8D%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E9%A2%84%E6%B5%8B/" title="No title">No title</a><time datetime="2022-08-16T09:18:55.589Z" title="Created 2022-08-16 17:18:55">2022-08-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/public/2022/08/16/2021-10-02-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-03-MCMC/" title="No title"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/public/img/404.jpg'" alt="No title"/></a><div class="content"><a class="title" href="/public/2022/08/16/2021-10-02-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-03-MCMC/" title="No title">No title</a><time datetime="2022-08-16T09:18:55.588Z" title="Created 2022-08-16 17:18:55">2022-08-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/public/2022/08/16/2021-10-02-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-02-DP/" title="No title"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/public/img/404.jpg'" alt="No title"/></a><div class="content"><a class="title" href="/public/2022/08/16/2021-10-02-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-02-DP/" title="No title">No title</a><time datetime="2022-08-16T09:18:55.586Z" title="Created 2022-08-16 17:18:55">2022-08-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/public/2022/08/16/2021-10-02-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-01-MDPs/" title="No title"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/public/img/404.jpg'" alt="No title"/></a><div class="content"><a class="title" href="/public/2022/08/16/2021-10-02-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-01-MDPs/" title="No title">No title</a><time datetime="2022-08-16T09:18:55.584Z" title="Created 2022-08-16 17:18:55">2022-08-16</time></div></div><div class="aside-list-item"><div class="content"><a class="title" href="/public/2022/08/16/hello-world/" title="Hello World">Hello World</a><time datetime="2022-08-16T07:18:01.370Z" title="Created 2022-08-16 15:18:01">2022-08-16</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022 By John Doe</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/public/js/utils.js"></script><script src="/public/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>